{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOTA group meetings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzwbmCQBo2gmqaQ6RRI9CR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/Meetings%2520and%2520communication/SOTA_group_meetings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrA7fJD9jz-",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "- different scoring methods produce different similarity evaluations\n",
        "- No single metric has been found to produce human level similarity scores between pairs of sentences.\n",
        "- nevertheless, the different imperfect similarity scores had had some sucsses in producing generative models.\n",
        "- often, human evalators produce incongrouent similarity scores according to different aspects of the text.<br>\n",
        "\n",
        "**what are we suggesting?**<br>\n",
        "- The abstract concept \"Semantics\" is actually an umberella term that can be broken down to different core-terms. linguistic research have tried to define the key aspect of the main abstract concept<br>\n",
        "- The limited succsess of generative models, that have used one or more of the existing evaluation metrics, can be attributed to the assumption that each sub division of semanticsd correlates with a different aspect of semantics.<br>\n",
        "- The relations between the sub-concepts can be learned by non-linear networks (e.g NN's)<br>\n",
        "<br>\n",
        "\n",
        "our proposal is as follows:<br>\n",
        "- create a NN that predicts human labels of similarity scores based on the different existing similarity evaluation metrics. \n",
        "if we see that we find that different metrics produce an overall improvement we will go on to the second phase:\n",
        "- using existing linguistic theoretical framework, we will break the umbrella concept \"Semantics\" into sub-concepts\n",
        "- label pairs of sentences according to those sub divisions\n",
        "- attribute different aspects of semantics to different similarity scores by calculating correlations scores and human labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-VqT9cmtDt0",
        "colab_type": "text"
      },
      "source": [
        "# NLP group meeting 07.06.2020\n",
        "Hernan, Shaul, Lee, Adam, Hezi\n",
        "\n",
        "### exploring non linearities between different components of the term semantics\n",
        "  - Theoretical frame work:\n",
        "  trying to break down the term semantics to sub-components based on existing research\n",
        "  - learning non-linear relations \n",
        "\n",
        "1. get different similarities scores between the sentences in the datasets\n",
        "2. try to use NN's with the different similarities scores as features and the Human similarities scores as labels\n",
        "3. exploring linguistic research to break down Sematics into different sub-terms \n",
        "\n",
        "**Notes**\n",
        "- should we take all simlaritiy metrics?\n",
        "- We're all starting with the same dataset: paraphrase.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L7aGzd1v9IC",
        "colab_type": "text"
      },
      "source": [
        "# Work division\n",
        "## Similarities calculation\n",
        "- implementation pipeline for each similarity metric might be a bit complicated\n",
        "we will want to split the coding implemintation to groups based on the mutual feature of each metric: <br>\n",
        " - group 1 (embedding based similarity scores):<br>\n",
        "POS distance (Lee), cosine similarities(Glove, fast-text) (Hezi) , L2 (elmo based)(Adam), WMD (Hernanm), BERT(Shaul)\n",
        "\n",
        " - group 2 (n-gram based scores):<br>\n",
        "word-overlap (Hernan), chr-f(Shaul), BLEU(Hezi), ROUGE(Lee) \n",
        "\n",
        "## Tehoretical background research\n",
        "\n",
        "## NN Implemintation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DeJOnFctDv9",
        "colab_type": "text"
      },
      "source": [
        "# Useful links\n",
        "https://pypi.org/project/bert-score/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvXoQsiZtDyn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}