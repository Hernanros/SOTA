{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNn5h5I/nS0tA2CMwTQZ0rf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/notebooks/nonlinear/%5BSS%5Dbase_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtlE_H4Xn0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, urllib, glob, sys\n",
        "from getpass import getpass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t284jQNGXs6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c18731ad-a20e-4488-bee0-17c60e77b11f"
      },
      "source": [
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "cmd_string = \"! git clone https://{0}:{1}@github.com/Hernanros/SOTA\".format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "\n",
        "%cd SOTA/data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: ShaulSolomon\n",
            "Password: ··········\n",
            "/content/SOTA/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdGj46LSX5aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"labeled_data.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByARqzZKX9l2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=[\"Unnamed: 0\",\"text_1\",\"text_2\"],inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxtCY1wCa40M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put label at the end of the df\n",
        "cols = df.columns.to_list()\n",
        "cols.remove(\"label\")\n",
        "cols.append(\"label\")\n",
        "df = df[cols]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig2pRdjeYvau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DS(Dataset):\n",
        "    def __init__(self,df):\n",
        "        super().__init__()\n",
        "        self.df = np.array(df.iloc[:,:-1])\n",
        "        self.labels = np.array(df.iloc[:,-1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.df[idx,:]\n",
        "        label = self.labels[idx]        \n",
        "\n",
        "        return feat,label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aiC_l6Ic5pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Basemodel(nn.Module):\n",
        "  \n",
        "  def __init__(self,n_feature,n_hidden,n_output, keep_probab = 0.1):\n",
        "    '''\n",
        "    input : tensor of dimensions (batch_size*n_feature)\n",
        "    output: tensor of dimension (batchsize*1)\n",
        "    '''\n",
        "    super().__init__()\n",
        "  \n",
        "    self.input_dim = n_feature    \n",
        "    self.hidden = nn.Linear(n_feature, n_hidden) \n",
        "    self.predict = torch.nn.Linear(n_hidden, n_output)\n",
        "    self.dropout = nn.Dropout(keep_probab)\n",
        "    # self.pool = nn.MaxPool2d(2, 2)\n",
        "    # self.norm = nn.BatchNorm2d(self.num_filters)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.dropout(self.hidden(x)))\n",
        "    x = self.predict(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A49n8DmfLAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = len(cols)-1\n",
        "num_hl = 128\n",
        "num_output = 1\n",
        "\n",
        "DATA_SIZE = df.shape[0]\n",
        "PERC_TRAIN = 0.8\n",
        "PERC_TEST = 1 - PERC_TRAIN\n",
        "TRAIN_SIZE = int(DATA_SIZE*PERC_TRAIN)\n",
        "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
        "\n",
        "model = Basemodel(num_features,num_hl,num_output)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7KKQExmfglk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(tr_loader,model,criterion,optimizer, num_epochs):\n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "    \n",
        "    \n",
        "    training_log =[]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"started training epoch no. {}\".format(epoch+1))\n",
        "      tr_loss = 0\n",
        "      for step,batch in enumerate(tr_loader):\n",
        "            feats,labels = batch\n",
        "            feats = feats.to(device,dtype=torch.float32)\n",
        "            labels = labels.to(device,dtype=torch.float32)\n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            tr_loss+=loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "      training_log.append({\n",
        "                'epoch':epoch,\n",
        "                'train_loss':tr_loss / len(tr_loader),\n",
        "                })\n",
        "      \n",
        "    return training_log\n",
        "  \n",
        "def test_evaluation(tst_loader,model,criterion): \n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "     \n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "\n",
        "    for step,batch in enumerate(tst_loader):\n",
        "        feats, labels = batch\n",
        "      \n",
        "        feats = feats.to(device,dtype=torch.float32)\n",
        "        labels = labels.to(device,dtype=torch.float32)\n",
        "        outputs = model(feats)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    return test_loss / TEST_SIZE"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQCRfxBJmrDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_list = list(range(df.shape[0]))\n",
        "np.random.seed(43)\n",
        "np.random.shuffle(rand_list)\n",
        "train_idx = rand_list[:int(len(rand_list)*PERC_TRAIN)]\n",
        "test_idx = rand_list[int(len(rand_list)*PERC_TEST):]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTGZw9QigqW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = DS(df.iloc[train_idx,:])\n",
        "test_set = DS(df.iloc[test_idx,:])\n",
        "train_loader=DataLoader(dataset= train_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "test_loader=DataLoader(dataset= test_set, batch_size = 4, shuffle = True, num_workers = 2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfJUFOJNgmlQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2448f61-2ff2-4f4b-ad3d-1a038139a8a6"
      },
      "source": [
        "train_epoch(train_loader,model,criterion,optimizer,num_epochs= 200)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 101\n",
            "started training epoch no. 102\n",
            "started training epoch no. 103\n",
            "started training epoch no. 104\n",
            "started training epoch no. 105\n",
            "started training epoch no. 106\n",
            "started training epoch no. 107\n",
            "started training epoch no. 108\n",
            "started training epoch no. 109\n",
            "started training epoch no. 110\n",
            "started training epoch no. 111\n",
            "started training epoch no. 112\n",
            "started training epoch no. 113\n",
            "started training epoch no. 114\n",
            "started training epoch no. 115\n",
            "started training epoch no. 116\n",
            "started training epoch no. 117\n",
            "started training epoch no. 118\n",
            "started training epoch no. 119\n",
            "started training epoch no. 120\n",
            "started training epoch no. 121\n",
            "started training epoch no. 122\n",
            "started training epoch no. 123\n",
            "started training epoch no. 124\n",
            "started training epoch no. 125\n",
            "started training epoch no. 126\n",
            "started training epoch no. 127\n",
            "started training epoch no. 128\n",
            "started training epoch no. 129\n",
            "started training epoch no. 130\n",
            "started training epoch no. 131\n",
            "started training epoch no. 132\n",
            "started training epoch no. 133\n",
            "started training epoch no. 134\n",
            "started training epoch no. 135\n",
            "started training epoch no. 136\n",
            "started training epoch no. 137\n",
            "started training epoch no. 138\n",
            "started training epoch no. 139\n",
            "started training epoch no. 140\n",
            "started training epoch no. 141\n",
            "started training epoch no. 142\n",
            "started training epoch no. 143\n",
            "started training epoch no. 144\n",
            "started training epoch no. 145\n",
            "started training epoch no. 146\n",
            "started training epoch no. 147\n",
            "started training epoch no. 148\n",
            "started training epoch no. 149\n",
            "started training epoch no. 150\n",
            "started training epoch no. 151\n",
            "started training epoch no. 152\n",
            "started training epoch no. 153\n",
            "started training epoch no. 154\n",
            "started training epoch no. 155\n",
            "started training epoch no. 156\n",
            "started training epoch no. 157\n",
            "started training epoch no. 158\n",
            "started training epoch no. 159\n",
            "started training epoch no. 160\n",
            "started training epoch no. 161\n",
            "started training epoch no. 162\n",
            "started training epoch no. 163\n",
            "started training epoch no. 164\n",
            "started training epoch no. 165\n",
            "started training epoch no. 166\n",
            "started training epoch no. 167\n",
            "started training epoch no. 168\n",
            "started training epoch no. 169\n",
            "started training epoch no. 170\n",
            "started training epoch no. 171\n",
            "started training epoch no. 172\n",
            "started training epoch no. 173\n",
            "started training epoch no. 174\n",
            "started training epoch no. 175\n",
            "started training epoch no. 176\n",
            "started training epoch no. 177\n",
            "started training epoch no. 178\n",
            "started training epoch no. 179\n",
            "started training epoch no. 180\n",
            "started training epoch no. 181\n",
            "started training epoch no. 182\n",
            "started training epoch no. 183\n",
            "started training epoch no. 184\n",
            "started training epoch no. 185\n",
            "started training epoch no. 186\n",
            "started training epoch no. 187\n",
            "started training epoch no. 188\n",
            "started training epoch no. 189\n",
            "started training epoch no. 190\n",
            "started training epoch no. 191\n",
            "started training epoch no. 192\n",
            "started training epoch no. 193\n",
            "started training epoch no. 194\n",
            "started training epoch no. 195\n",
            "started training epoch no. 196\n",
            "started training epoch no. 197\n",
            "started training epoch no. 198\n",
            "started training epoch no. 199\n",
            "started training epoch no. 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'epoch': 0, 'train_loss': 0.8859830445051193},\n",
              " {'epoch': 1, 'train_loss': 0.6977964454889297},\n",
              " {'epoch': 2, 'train_loss': 0.6998442320525646},\n",
              " {'epoch': 3, 'train_loss': 0.6949249115586281},\n",
              " {'epoch': 4, 'train_loss': 0.6942613966763019},\n",
              " {'epoch': 5, 'train_loss': 0.6863221144676208},\n",
              " {'epoch': 6, 'train_loss': 0.6780826434493065},\n",
              " {'epoch': 7, 'train_loss': 0.6935897462069989},\n",
              " {'epoch': 8, 'train_loss': 0.6806354510784149},\n",
              " {'epoch': 9, 'train_loss': 0.6847915238142014},\n",
              " {'epoch': 10, 'train_loss': 0.6927295958995819},\n",
              " {'epoch': 11, 'train_loss': 0.6891531473398209},\n",
              " {'epoch': 12, 'train_loss': 0.6757026213407517},\n",
              " {'epoch': 13, 'train_loss': 0.6942389181256294},\n",
              " {'epoch': 14, 'train_loss': 0.6850704678893089},\n",
              " {'epoch': 15, 'train_loss': 0.7017373426258564},\n",
              " {'epoch': 16, 'train_loss': 0.6822302600741387},\n",
              " {'epoch': 17, 'train_loss': 0.674426079839468},\n",
              " {'epoch': 18, 'train_loss': 0.6770938548445702},\n",
              " {'epoch': 19, 'train_loss': 0.6790618096292019},\n",
              " {'epoch': 20, 'train_loss': 0.6913367757201194},\n",
              " {'epoch': 21, 'train_loss': 0.6732734099030495},\n",
              " {'epoch': 22, 'train_loss': 0.6787734772264957},\n",
              " {'epoch': 23, 'train_loss': 0.6750275874137879},\n",
              " {'epoch': 24, 'train_loss': 0.6754159630835056},\n",
              " {'epoch': 25, 'train_loss': 0.6854340142011642},\n",
              " {'epoch': 26, 'train_loss': 0.6761639907956123},\n",
              " {'epoch': 27, 'train_loss': 0.6746966490149497},\n",
              " {'epoch': 28, 'train_loss': 0.6735596038401127},\n",
              " {'epoch': 29, 'train_loss': 0.674922239780426},\n",
              " {'epoch': 30, 'train_loss': 0.6720125834643841},\n",
              " {'epoch': 31, 'train_loss': 0.667767236828804},\n",
              " {'epoch': 32, 'train_loss': 0.6752425682544708},\n",
              " {'epoch': 33, 'train_loss': 0.6677108183503151},\n",
              " {'epoch': 34, 'train_loss': 0.6727337108552456},\n",
              " {'epoch': 35, 'train_loss': 0.6703963552415371},\n",
              " {'epoch': 36, 'train_loss': 0.6771132929623127},\n",
              " {'epoch': 37, 'train_loss': 0.6763244332373142},\n",
              " {'epoch': 38, 'train_loss': 0.6719371958076954},\n",
              " {'epoch': 39, 'train_loss': 0.6732781693339348},\n",
              " {'epoch': 40, 'train_loss': 0.6701919977366925},\n",
              " {'epoch': 41, 'train_loss': 0.666798335313797},\n",
              " {'epoch': 42, 'train_loss': 0.6883202275633812},\n",
              " {'epoch': 43, 'train_loss': 0.6681370855867863},\n",
              " {'epoch': 44, 'train_loss': 0.6697034065425396},\n",
              " {'epoch': 45, 'train_loss': 0.665309705734253},\n",
              " {'epoch': 46, 'train_loss': 0.6676913420855999},\n",
              " {'epoch': 47, 'train_loss': 0.6737328483164311},\n",
              " {'epoch': 48, 'train_loss': 0.6765382380783558},\n",
              " {'epoch': 49, 'train_loss': 0.66706583365798},\n",
              " {'epoch': 50, 'train_loss': 0.6722530898451805},\n",
              " {'epoch': 51, 'train_loss': 0.6637301620841026},\n",
              " {'epoch': 52, 'train_loss': 0.6781324425339699},\n",
              " {'epoch': 53, 'train_loss': 0.6644368416070938},\n",
              " {'epoch': 54, 'train_loss': 0.6651660740375519},\n",
              " {'epoch': 55, 'train_loss': 0.6767619422078133},\n",
              " {'epoch': 56, 'train_loss': 0.6666114105284214},\n",
              " {'epoch': 57, 'train_loss': 0.6760238337516785},\n",
              " {'epoch': 58, 'train_loss': 0.6681557711958885},\n",
              " {'epoch': 59, 'train_loss': 0.6658001953363418},\n",
              " {'epoch': 60, 'train_loss': 0.6605856728553772},\n",
              " {'epoch': 61, 'train_loss': 0.6696317078173161},\n",
              " {'epoch': 62, 'train_loss': 0.674456819742918},\n",
              " {'epoch': 63, 'train_loss': 0.6700342339277268},\n",
              " {'epoch': 64, 'train_loss': 0.6767401899397373},\n",
              " {'epoch': 65, 'train_loss': 0.6722306224703789},\n",
              " {'epoch': 66, 'train_loss': 0.6683899493515492},\n",
              " {'epoch': 67, 'train_loss': 0.6667636317014695},\n",
              " {'epoch': 68, 'train_loss': 0.6657235790789128},\n",
              " {'epoch': 69, 'train_loss': 0.6644904287159443},\n",
              " {'epoch': 70, 'train_loss': 0.6719850233197212},\n",
              " {'epoch': 71, 'train_loss': 0.6657863131165505},\n",
              " {'epoch': 72, 'train_loss': 0.6621853558719158},\n",
              " {'epoch': 73, 'train_loss': 0.6714280219376088},\n",
              " {'epoch': 74, 'train_loss': 0.6638081035017968},\n",
              " {'epoch': 75, 'train_loss': 0.662125054448843},\n",
              " {'epoch': 76, 'train_loss': 0.6713774728775025},\n",
              " {'epoch': 77, 'train_loss': 0.666991006731987},\n",
              " {'epoch': 78, 'train_loss': 0.6665591016411782},\n",
              " {'epoch': 79, 'train_loss': 0.667179444283247},\n",
              " {'epoch': 80, 'train_loss': 0.6647487543523312},\n",
              " {'epoch': 81, 'train_loss': 0.6659177695214749},\n",
              " {'epoch': 82, 'train_loss': 0.6770265263319015},\n",
              " {'epoch': 83, 'train_loss': 0.6674925395846367},\n",
              " {'epoch': 84, 'train_loss': 0.6595596079528332},\n",
              " {'epoch': 85, 'train_loss': 0.6709485100209713},\n",
              " {'epoch': 86, 'train_loss': 0.6585978545248509},\n",
              " {'epoch': 87, 'train_loss': 0.6704615172743797},\n",
              " {'epoch': 88, 'train_loss': 0.6735504513978958},\n",
              " {'epoch': 89, 'train_loss': 0.6614522452652455},\n",
              " {'epoch': 90, 'train_loss': 0.6716875560581684},\n",
              " {'epoch': 91, 'train_loss': 0.6693172958493233},\n",
              " {'epoch': 92, 'train_loss': 0.6610142184793949},\n",
              " {'epoch': 93, 'train_loss': 0.6709870177507401},\n",
              " {'epoch': 94, 'train_loss': 0.6668050363659859},\n",
              " {'epoch': 95, 'train_loss': 0.6682833154499531},\n",
              " {'epoch': 96, 'train_loss': 0.6749347445368766},\n",
              " {'epoch': 97, 'train_loss': 0.6568692295253277},\n",
              " {'epoch': 98, 'train_loss': 0.6684536737203598},\n",
              " {'epoch': 99, 'train_loss': 0.6678783059120178},\n",
              " {'epoch': 100, 'train_loss': 0.6768157345056534},\n",
              " {'epoch': 101, 'train_loss': 0.6699132730066776},\n",
              " {'epoch': 102, 'train_loss': 0.6683400213718415},\n",
              " {'epoch': 103, 'train_loss': 0.6647422955930233},\n",
              " {'epoch': 104, 'train_loss': 0.673150084912777},\n",
              " {'epoch': 105, 'train_loss': 0.6658683420717716},\n",
              " {'epoch': 106, 'train_loss': 0.6728658743202687},\n",
              " {'epoch': 107, 'train_loss': 0.6629109583795071},\n",
              " {'epoch': 108, 'train_loss': 0.6662319648265839},\n",
              " {'epoch': 109, 'train_loss': 0.6675630317628384},\n",
              " {'epoch': 110, 'train_loss': 0.6610095219314098},\n",
              " {'epoch': 111, 'train_loss': 0.670299726575613},\n",
              " {'epoch': 112, 'train_loss': 0.6631353498995304},\n",
              " {'epoch': 113, 'train_loss': 0.6590674617886543},\n",
              " {'epoch': 114, 'train_loss': 0.663933908045292},\n",
              " {'epoch': 115, 'train_loss': 0.6659630313515663},\n",
              " {'epoch': 116, 'train_loss': 0.669311438947916},\n",
              " {'epoch': 117, 'train_loss': 0.6579972164332867},\n",
              " {'epoch': 118, 'train_loss': 0.6599728538095951},\n",
              " {'epoch': 119, 'train_loss': 0.6671607990562916},\n",
              " {'epoch': 120, 'train_loss': 0.6580735421180726},\n",
              " {'epoch': 121, 'train_loss': 0.6617650797963143},\n",
              " {'epoch': 122, 'train_loss': 0.6820309612154961},\n",
              " {'epoch': 123, 'train_loss': 0.6672037640213966},\n",
              " {'epoch': 124, 'train_loss': 0.6644984476268292},\n",
              " {'epoch': 125, 'train_loss': 0.6543082474172115},\n",
              " {'epoch': 126, 'train_loss': 0.6648315294086933},\n",
              " {'epoch': 127, 'train_loss': 0.6670972852408886},\n",
              " {'epoch': 128, 'train_loss': 0.6700642308592797},\n",
              " {'epoch': 129, 'train_loss': 0.66578115940094},\n",
              " {'epoch': 130, 'train_loss': 0.6686393643915653},\n",
              " {'epoch': 131, 'train_loss': 0.657096262127161},\n",
              " {'epoch': 132, 'train_loss': 0.6640604288876056},\n",
              " {'epoch': 133, 'train_loss': 0.6717194607853889},\n",
              " {'epoch': 134, 'train_loss': 0.6615272632241249},\n",
              " {'epoch': 135, 'train_loss': 0.6598601730167866},\n",
              " {'epoch': 136, 'train_loss': 0.6699943560361862},\n",
              " {'epoch': 137, 'train_loss': 0.6712587529420853},\n",
              " {'epoch': 138, 'train_loss': 0.6681272360682488},\n",
              " {'epoch': 139, 'train_loss': 0.663918857127428},\n",
              " {'epoch': 140, 'train_loss': 0.6646184885501861},\n",
              " {'epoch': 141, 'train_loss': 0.6628326870501041},\n",
              " {'epoch': 142, 'train_loss': 0.6629545518755913},\n",
              " {'epoch': 143, 'train_loss': 0.6612525953352452},\n",
              " {'epoch': 144, 'train_loss': 0.657796491086483},\n",
              " {'epoch': 145, 'train_loss': 0.6659454697370529},\n",
              " {'epoch': 146, 'train_loss': 0.6655227622389793},\n",
              " {'epoch': 147, 'train_loss': 0.6658045816421508},\n",
              " {'epoch': 148, 'train_loss': 0.6643233208358288},\n",
              " {'epoch': 149, 'train_loss': 0.6668288652598858},\n",
              " {'epoch': 150, 'train_loss': 0.6614024694263935},\n",
              " {'epoch': 151, 'train_loss': 0.6625493970513344},\n",
              " {'epoch': 152, 'train_loss': 0.6647877909243107},\n",
              " {'epoch': 153, 'train_loss': 0.672091886550188},\n",
              " {'epoch': 154, 'train_loss': 0.6654132734239101},\n",
              " {'epoch': 155, 'train_loss': 0.6643571618199349},\n",
              " {'epoch': 156, 'train_loss': 0.6650314213335514},\n",
              " {'epoch': 157, 'train_loss': 0.6594237054884434},\n",
              " {'epoch': 158, 'train_loss': 0.663347789645195},\n",
              " {'epoch': 159, 'train_loss': 0.6593918903172016},\n",
              " {'epoch': 160, 'train_loss': 0.6572189953923225},\n",
              " {'epoch': 161, 'train_loss': 0.6583202850818634},\n",
              " {'epoch': 162, 'train_loss': 0.6554916815459728},\n",
              " {'epoch': 163, 'train_loss': 0.6635643330216408},\n",
              " {'epoch': 164, 'train_loss': 0.6578043150901794},\n",
              " {'epoch': 165, 'train_loss': 0.6627527287602425},\n",
              " {'epoch': 166, 'train_loss': 0.664779764264822},\n",
              " {'epoch': 167, 'train_loss': 0.6551310107111931},\n",
              " {'epoch': 168, 'train_loss': 0.6519428452849388},\n",
              " {'epoch': 169, 'train_loss': 0.6730194725096226},\n",
              " {'epoch': 170, 'train_loss': 0.6656931069493294},\n",
              " {'epoch': 171, 'train_loss': 0.6597033905982971},\n",
              " {'epoch': 172, 'train_loss': 0.6678857734799385},\n",
              " {'epoch': 173, 'train_loss': 0.6600939160585404},\n",
              " {'epoch': 174, 'train_loss': 0.6717475821077824},\n",
              " {'epoch': 175, 'train_loss': 0.6629875372350216},\n",
              " {'epoch': 176, 'train_loss': 0.6589128588140011},\n",
              " {'epoch': 177, 'train_loss': 0.6757926113903523},\n",
              " {'epoch': 178, 'train_loss': 0.6666763000190258},\n",
              " {'epoch': 179, 'train_loss': 0.6680407726764679},\n",
              " {'epoch': 180, 'train_loss': 0.6596206621825695},\n",
              " {'epoch': 181, 'train_loss': 0.6622168143093586},\n",
              " {'epoch': 182, 'train_loss': 0.6565996088087559},\n",
              " {'epoch': 183, 'train_loss': 0.6565685024857522},\n",
              " {'epoch': 184, 'train_loss': 0.6593310064077378},\n",
              " {'epoch': 185, 'train_loss': 0.6651045438647271},\n",
              " {'epoch': 186, 'train_loss': 0.6604108531773091},\n",
              " {'epoch': 187, 'train_loss': 0.6543782867491246},\n",
              " {'epoch': 188, 'train_loss': 0.6613336965441704},\n",
              " {'epoch': 189, 'train_loss': 0.6515954765677452},\n",
              " {'epoch': 190, 'train_loss': 0.655981868058443},\n",
              " {'epoch': 191, 'train_loss': 0.6617373923957348},\n",
              " {'epoch': 192, 'train_loss': 0.6543188244104385},\n",
              " {'epoch': 193, 'train_loss': 0.6743127463757992},\n",
              " {'epoch': 194, 'train_loss': 0.6653990885615348},\n",
              " {'epoch': 195, 'train_loss': 0.6662698046863079},\n",
              " {'epoch': 196, 'train_loss': 0.6584448161721229},\n",
              " {'epoch': 197, 'train_loss': 0.6522920280694962},\n",
              " {'epoch': 198, 'train_loss': 0.6591369387507439},\n",
              " {'epoch': 199, 'train_loss': 0.6588799753785133}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4VAdaGYgpKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f5f86917-2dd2-407a-cce1-19528b4a7798"
      },
      "source": [
        "test_evaluation(test_loader,model,criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6450862105190754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXivmviE7Fp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8ad434f0-30d2-4e45-f7cc-3164adb12c92"
      },
      "source": [
        "df.columns[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['POS dist score', '1-gram_overlap', 'chrf_score_norm', 'WMD',\n",
              "       'ROUGE-1 recall', 'ROUGE-1 precision', 'ROUGE-1 F', 'ROUGE-2 recall',\n",
              "       'ROUGE-2 precision', 'ROUGE-2 F', 'ROUGE-L recall', 'ROUGE-L precision',\n",
              "       'ROUGE-L F', 'BertScore', 'L2_score'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF0PDkL7qHod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = pd.DataFrame(df.columns[:-1], columns=[\"metric\"])\n",
        "features['weights'] = model.hidden.weight.data.cpu().numpy()[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sgojICu7Pi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "cb2b92b0-d26d-493e-dce2-b7554bbc87a9"
      },
      "source": [
        "features.sort_values(by=['weights'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POS dist score</td>\n",
              "      <td>-0.505415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WMD</td>\n",
              "      <td>-0.405754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>L2_score</td>\n",
              "      <td>-0.279422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ROUGE-2 F</td>\n",
              "      <td>-0.214183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-gram_overlap</td>\n",
              "      <td>-0.142205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ROUGE-1 recall</td>\n",
              "      <td>-0.110483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ROUGE-2 precision</td>\n",
              "      <td>-0.104712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ROUGE-L recall</td>\n",
              "      <td>-0.036120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ROUGE-L precision</td>\n",
              "      <td>-0.023660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ROUGE-1 F</td>\n",
              "      <td>-0.007301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ROUGE-L F</td>\n",
              "      <td>-0.003294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ROUGE-2 recall</td>\n",
              "      <td>0.087960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BertScore</td>\n",
              "      <td>0.127144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chrf_score_norm</td>\n",
              "      <td>0.138929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ROUGE-1 precision</td>\n",
              "      <td>0.196650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               metric   weights\n",
              "0      POS dist score -0.505415\n",
              "3                 WMD -0.405754\n",
              "14           L2_score -0.279422\n",
              "9           ROUGE-2 F -0.214183\n",
              "1      1-gram_overlap -0.142205\n",
              "4      ROUGE-1 recall -0.110483\n",
              "8   ROUGE-2 precision -0.104712\n",
              "10     ROUGE-L recall -0.036120\n",
              "11  ROUGE-L precision -0.023660\n",
              "6           ROUGE-1 F -0.007301\n",
              "12          ROUGE-L F -0.003294\n",
              "7      ROUGE-2 recall  0.087960\n",
              "13          BertScore  0.127144\n",
              "2     chrf_score_norm  0.138929\n",
              "5   ROUGE-1 precision  0.196650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTb8epI7Sk6k",
        "colab_type": "text"
      },
      "source": [
        "## Get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "201UEZTNsTRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = np.array(df.iloc[:,:-1])\n",
        "labels = np.array(df.iloc[:,-1])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMXXk8YsHf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.has_cuda:\n",
        "  device = torch.device('cuda:0')\n",
        "  model.to(device)\n",
        "else:\n",
        "  device = torch.device('cpu:0')\n",
        "\n",
        "predictions = model(torch.tensor(features, dtype=torch.float32).to(device))\n",
        "predictions  = predictions.cpu().detach().numpy()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kay6qbrjuIt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred = pd.read_csv(\"/content/SOTA/data/Paraphrase_labeled_data_with_predictions.csv\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weUwyqYSuizY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred['MLP predictions'] = pd.Series(predictions.reshape(-1))\n",
        "df_pred.to_csv(\"Paraphrase_labeled_data_with_predictions_both.csv\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVb0X_zuqne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3ffbc8d3-dcd2-42d1-dae6-d28a58b2dea4"
      },
      "source": [
        "\n",
        "!git add ./Paraphrase_labeled_data_with_predictions_both.csv\n",
        "!git config --global user.email \"shaulsolomon@gmail.com\"\n",
        "!git config --global user.name \"Shaul Solomon\"\n",
        "!git commit -m \"Added MLP pred\"\n",
        "!git push"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master 5f54665] Added MLP pred\n",
            " 1 file changed, 999 insertions(+)\n",
            " create mode 100644 data/Paraphrase_labeled_data_with_predictions_both.csv\n",
            "Counting objects: 4, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 55.88 KiB | 5.08 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/Hernanros/SOTA\n",
            "   6c10cc4..5f54665  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}