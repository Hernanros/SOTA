{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPG9FDGJsJgGY/zMsyfNmmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/notebooks/nonlinear/%5BSS%5Dprediction%20scores%20for%20whole%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AM3BB0wTb0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, urllib, glob, sys\n",
        "from getpass import getpass\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4mQSwqTeDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b5fe55cf-c927-4040-cc70-35e5f542c03e"
      },
      "source": [
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "cmd_string = \"! git clone https://{0}:{1}@github.com/Hernanros/SOTA\".format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "\n",
        "%cd SOTA/data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: ShaulSolomon\n",
            "Password: ··········\n",
            "/content/SOTA/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNP0q4WpTfj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_pickle(\"/content/SOTA/data/combined_data_no_nans.pickle\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L5ARVdDT5xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "3bda3135-8bec-46d2-e04b-ad7c99d02a29"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>bleu_allwords</th>\n",
              "      <th>bleu_withoutstop</th>\n",
              "      <th>glove_allwords</th>\n",
              "      <th>glove_withoutstop</th>\n",
              "      <th>ftext_allwords</th>\n",
              "      <th>ftext_withoutstop</th>\n",
              "      <th>WMD</th>\n",
              "      <th>1-gram_overlap</th>\n",
              "      <th>2-gram_overlap</th>\n",
              "      <th>3-gram_overlap</th>\n",
              "      <th>4-gram_overlap</th>\n",
              "      <th>ROUGE-1 recall</th>\n",
              "      <th>ROUGE-1 precision</th>\n",
              "      <th>ROUGE-1 F</th>\n",
              "      <th>ROUGE-2 recall</th>\n",
              "      <th>ROUGE-2 precision</th>\n",
              "      <th>ROUGE-2 F</th>\n",
              "      <th>ROUGE-L recall</th>\n",
              "      <th>ROUGE-L precision</th>\n",
              "      <th>ROUGE-L F</th>\n",
              "      <th>chrf_score</th>\n",
              "      <th>chrf_score_norm</th>\n",
              "      <th>POS dist score</th>\n",
              "      <th>text_1_tokens</th>\n",
              "      <th>text_2_tokens</th>\n",
              "      <th>L2_score</th>\n",
              "      <th>bert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012.MSRpar.test.tsv</td>\n",
              "      <td>4.4</td>\n",
              "      <td>The problem likely will mean corrective change...</td>\n",
              "      <td>He said the problem needs to be corrected befo...</td>\n",
              "      <td>0.375739</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>96.2</td>\n",
              "      <td>90.82</td>\n",
              "      <td>77.23</td>\n",
              "      <td>77.39</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.536815</td>\n",
              "      <td>2.684077</td>\n",
              "      <td>3.055075</td>\n",
              "      <td>[The, problem, likely, will, mean, corrective,...</td>\n",
              "      <td>[He, said, the, problem, needs, to, be, correc...</td>\n",
              "      <td>10.527886</td>\n",
              "      <td>0.926813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                dataset  label  ...   L2_score      bert\n",
              "0  2012.MSRpar.test.tsv    4.4  ...  10.527886  0.926813\n",
              "\n",
              "[1 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upJsAJtiUCSY",
        "colab_type": "text"
      },
      "source": [
        "# Running MLP Model on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML1VU342UGSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all = df.drop(columns=[\"dataset\",\"text_1\",\"text_2\",\"text_1_tokens\",\"text_2_tokens\"])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vz_ZhZqUXMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put label at the end of the df\n",
        "cols = df_all.columns.to_list()\n",
        "cols.remove(\"label\")\n",
        "cols.append(\"label\")\n",
        "df_all = df_all[cols]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9a2Nnm_UjKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DS(Dataset):\n",
        "    def __init__(self,df):\n",
        "        super().__init__()\n",
        "        self.df = np.array(df.iloc[:,:-1])\n",
        "        self.labels = np.array(df.iloc[:,-1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.df[idx,:]\n",
        "        label = self.labels[idx]        \n",
        "\n",
        "        return feat,label"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLS8se-UltF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Basemodel(nn.Module):\n",
        "  \n",
        "  def __init__(self,n_feature,n_hidden,n_output, keep_probab = 0.1):\n",
        "    '''\n",
        "    input : tensor of dimensions (batch_size*n_feature)\n",
        "    output: tensor of dimension (batchsize*1)\n",
        "    '''\n",
        "    super().__init__()\n",
        "  \n",
        "    self.input_dim = n_feature    \n",
        "    self.hidden = nn.Linear(n_feature, n_hidden) \n",
        "    self.predict = torch.nn.Linear(n_hidden, n_output)\n",
        "    self.dropout = nn.Dropout(keep_probab)\n",
        "    # self.pool = nn.MaxPool2d(2, 2)\n",
        "    # self.norm = nn.BatchNorm2d(self.num_filters)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.dropout(self.hidden(x)))\n",
        "    x = self.predict(x)\n",
        "    return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8i_cBtLUozZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = len(cols)-1\n",
        "num_hl = 128\n",
        "num_output = 1\n",
        "\n",
        "DATA_SIZE = df_all.shape[0]\n",
        "PERC_TRAIN = 0.8\n",
        "PERC_TEST = 1 - PERC_TRAIN\n",
        "TRAIN_SIZE = int(DATA_SIZE*PERC_TRAIN)\n",
        "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
        "\n",
        "model = Basemodel(num_features,num_hl,num_output)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvJiajlUUt9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(tr_loader,model,criterion,optimizer, num_epochs):\n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "    \n",
        "    \n",
        "    training_log =[]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"started training epoch no. {}\".format(epoch+1))\n",
        "      tr_loss = 0\n",
        "      for step,batch in enumerate(tr_loader):\n",
        "            feats,labels = batch\n",
        "            feats = feats.to(device,dtype=torch.float32)\n",
        "            labels = labels.to(device,dtype=torch.float32)\n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            tr_loss+=loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "      training_log.append({\n",
        "                'epoch':epoch,\n",
        "                'train_loss':tr_loss / len(tr_loader),\n",
        "                })\n",
        "      \n",
        "    return training_log\n",
        "  \n",
        "def test_evaluation(tst_loader,model,criterion): \n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "     \n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "\n",
        "    for step,batch in enumerate(tst_loader):\n",
        "        feats, labels = batch\n",
        "      \n",
        "        feats = feats.to(device,dtype=torch.float32)\n",
        "        labels = labels.to(device,dtype=torch.float32)\n",
        "        outputs = model(feats)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    return test_loss / TEST_SIZE"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFdXs8FUvsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_list = list(range(df_all.shape[0]))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(rand_list)\n",
        "train_idx = rand_list[:int(len(rand_list)*PERC_TRAIN)]\n",
        "test_idx = rand_list[int(len(rand_list)*PERC_TEST):]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0py9jHkqUzdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = DS(df_all.iloc[train_idx,:])\n",
        "test_set = DS(df_all.iloc[test_idx,:])\n",
        "train_loader=DataLoader(dataset= train_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "test_loader=DataLoader(dataset= test_set, batch_size = 4, shuffle = True, num_workers = 2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OraXdeBuUz6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c3b020c-f1f1-470c-c9d6-f74e92b0b386"
      },
      "source": [
        "train_epoch(train_loader,model,criterion,optimizer,num_epochs= 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 101\n",
            "started training epoch no. 102\n",
            "started training epoch no. 103\n",
            "started training epoch no. 104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruCoMg8JYvOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_evaluation(test_loader,model,criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR30VFS0WWiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = np.array(df_all.iloc[:,:-1])\n",
        "labels = np.array(df_all.iloc[:,-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86cm4C6U1dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.has_cuda:\n",
        "  device = torch.device('cuda:0')\n",
        "  model.to(device)\n",
        "else:\n",
        "  device = torch.device('cpu:0')\n",
        "\n",
        "predictions = model(torch.tensor(features, dtype=torch.float32).to(device))\n",
        "predictions  = predictions.cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8aOzM-cWdow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred = df.copy()\n",
        "df_pred['MLP predictions'] = pd.Series(predictions.reshape(-1))\n",
        "df_pred.to_csv(\"combined_data_with_predictions_both.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD04MjKAWw3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!git add ./combined_data_with_predictions_both.csv\n",
        "!git config --global user.email \"shaulsolomon@gmail.com\"\n",
        "!git config --global user.name \"Shaul Solomon\"\n",
        "!git commit -m \"Added MLP pred to whole dataset\"\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBhBDQJHW2VK",
        "colab_type": "text"
      },
      "source": [
        "# For each dataset seperately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hcx4CK7X-6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_each = df.drop(columns=[\"text_1\",\"text_2\",\"text_1_tokens\",\"text_2_tokens\"]).groupby(\"dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAmC0zYXAnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log = {}\n",
        "df_pred = pd.DataFrame([])\n",
        "\n",
        "for name, df_group in df_each:\n",
        "  num_features = df_group.shape[1]\n",
        "  num_hl = 128\n",
        "  num_output = 1\n",
        "\n",
        "  DATA_SIZE = df_group.shape[0]\n",
        "  PERC_TRAIN = 0.8\n",
        "  PERC_TEST = 1 - PERC_TRAIN\n",
        "  TRAIN_SIZE = int(DATA_SIZE*PERC_TRAIN)\n",
        "  TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
        "\n",
        "  model = Basemodel(num_features,num_hl,num_output)\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  rand_list = list(range(df_group.shape[0]))\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(rand_list)\n",
        "  train_idx = rand_list[:int(len(rand_list)*PERC_TRAIN)]\n",
        "  test_idx = rand_list[int(len(rand_list)*PERC_TEST):]\n",
        "  train_set = DS(df_group.iloc[train_idx,:])\n",
        "  test_set = DS(df_group.iloc[test_idx,:])\n",
        "\n",
        "  train_loader=DataLoader(dataset= train_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "  test_loader=DataLoader(dataset= test_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "\n",
        "  train_epoch(train_loader,model,criterion,optimizer,num_epochs= 200)\n",
        "  loss = test_evaluation(test_loader,model,criterion)\n",
        "  log[name] = loss\n",
        "\n",
        "  features = np.array(df_all.iloc[:,:-1])\n",
        "  labels = np.array(df_all.iloc[:,-1])\n",
        "\n",
        "  if torch.has_cuda:\n",
        "    device = torch.device('cuda:0')\n",
        "    model.to(device)\n",
        "  else:\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "  predictions = model(torch.tensor(features, dtype=torch.float32).to(device))\n",
        "  predictions  = predictions.cpu().detach().numpy()\n",
        "\n",
        "  df_pred[name]['MLP predictions'] = pd.Series(predictions.reshape(-1))\n",
        "\n",
        "print(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gjBcJvZZhI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Ld0HujZh_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}