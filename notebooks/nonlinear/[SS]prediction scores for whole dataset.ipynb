{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvzA/onHFZKB2vVSplxhYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/notebooks/nonlinear/%5BSS%5Dprediction%20scores%20for%20whole%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AM3BB0wTb0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, urllib, glob, sys\n",
        "from getpass import getpass\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4mQSwqTeDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f0d8a719-1842-4db6-b6df-ec10af9dc477"
      },
      "source": [
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "cmd_string = \"! git clone https://{0}:{1}@github.com/Hernanros/SOTA\".format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "\n",
        "%cd SOTA/data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: ShaulSolomon\n",
            "Password: ··········\n",
            "/content/SOTA/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNP0q4WpTfj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/SOTA/data/combined_data_with_predictions_on_separate_datasets.csv\")"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L5ARVdDT5xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "d2de74d4-47df-4a38-80ed-c78c5fe77479"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>bleu_allwords</th>\n",
              "      <th>bleu_withoutstop</th>\n",
              "      <th>glove_allwords</th>\n",
              "      <th>glove_withoutstop</th>\n",
              "      <th>ftext_allwords</th>\n",
              "      <th>ftext_withoutstop</th>\n",
              "      <th>WMD</th>\n",
              "      <th>1-gram_overlap</th>\n",
              "      <th>2-gram_overlap</th>\n",
              "      <th>3-gram_overlap</th>\n",
              "      <th>4-gram_overlap</th>\n",
              "      <th>ROUGE-1 recall</th>\n",
              "      <th>ROUGE-1 precision</th>\n",
              "      <th>ROUGE-1 F</th>\n",
              "      <th>ROUGE-2 recall</th>\n",
              "      <th>ROUGE-2 precision</th>\n",
              "      <th>ROUGE-2 F</th>\n",
              "      <th>ROUGE-L recall</th>\n",
              "      <th>ROUGE-L precision</th>\n",
              "      <th>ROUGE-L F</th>\n",
              "      <th>chrf_score</th>\n",
              "      <th>chrf_score_norm</th>\n",
              "      <th>POS dist score</th>\n",
              "      <th>text_1_tokens</th>\n",
              "      <th>text_2_tokens</th>\n",
              "      <th>L2_score</th>\n",
              "      <th>bert</th>\n",
              "      <th>Predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012.MSRpar.test.tsv</td>\n",
              "      <td>4.4</td>\n",
              "      <td>The problem likely will mean corrective change...</td>\n",
              "      <td>He said the problem needs to be corrected befo...</td>\n",
              "      <td>0.375739</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>96.2</td>\n",
              "      <td>90.82</td>\n",
              "      <td>77.23</td>\n",
              "      <td>77.39</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.536815</td>\n",
              "      <td>2.684077</td>\n",
              "      <td>3.055075</td>\n",
              "      <td>['The', 'problem', 'likely', 'will', 'mean', '...</td>\n",
              "      <td>['He', 'said', 'the', 'problem', 'needs', 'to'...</td>\n",
              "      <td>10.527886</td>\n",
              "      <td>0.926813</td>\n",
              "      <td>2.988756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                dataset  label  ...      bert Predictions\n",
              "0  2012.MSRpar.test.tsv    4.4  ...  0.926813    2.988756\n",
              "\n",
              "[1 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upJsAJtiUCSY",
        "colab_type": "text"
      },
      "source": [
        "# Running MLP Model on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML1VU342UGSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all = df.drop(columns=[\"dataset\",\"text_1\",\"text_2\",\"text_1_tokens\",\"text_2_tokens\",\"Predictions\"])"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vz_ZhZqUXMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put label at the end of the df\n",
        "cols = df_all.columns.to_list()\n",
        "cols.remove(\"label\")\n",
        "cols.append(\"label\")\n",
        "df_all = df_all[cols]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYwLfe_ncbX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3ba98da-5898-46b3-f4af-0ab951fde3fb"
      },
      "source": [
        "len(df_all.columns)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9a2Nnm_UjKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DS(Dataset):\n",
        "    def __init__(self,df):\n",
        "        super().__init__()\n",
        "        X = df.iloc[:,:-1] \n",
        "        column_names = list(X.columns) \n",
        "        x = X.values #returns a numpy array\n",
        "        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 5))\n",
        "        x_scaled = min_max_scaler.fit_transform(x)\n",
        "        self.df = np.array(pd.DataFrame(x_scaled, columns=column_names))\n",
        "        self.labels = np.array(df.iloc[:,-1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.df[idx,:]\n",
        "        label = self.labels[idx]        \n",
        "\n",
        "        return feat,label"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLS8se-UltF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Basemodel(nn.Module):\n",
        "  \n",
        "  def __init__(self,n_feature,n_hidden,n_output, keep_probab = 0.1):\n",
        "    '''\n",
        "    input : tensor of dimensions (batch_size*n_feature)\n",
        "    output: tensor of dimension (batchsize*1)\n",
        "    '''\n",
        "    super().__init__()\n",
        "  \n",
        "    self.input_dim = n_feature    \n",
        "    self.hidden = nn.Linear(n_feature, n_hidden) \n",
        "    self.predict = torch.nn.Linear(n_hidden, n_output)\n",
        "    self.dropout = nn.Dropout(keep_probab)\n",
        "    # self.pool = nn.MaxPool2d(2, 2)\n",
        "    # self.norm = nn.BatchNorm2d(self.num_filters)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.dropout(self.hidden(x)))\n",
        "    x = self.predict(x)\n",
        "    return x"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8i_cBtLUozZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = len(cols)-1\n",
        "num_hl = 128\n",
        "num_output = 1\n",
        "\n",
        "DATA_SIZE = df_all.shape[0]\n",
        "PERC_TRAIN = 0.8\n",
        "PERC_TEST = 1 - PERC_TRAIN\n",
        "TRAIN_SIZE = int(DATA_SIZE*PERC_TRAIN)\n",
        "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
        "\n",
        "model = Basemodel(num_features,num_hl,num_output)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvJiajlUUt9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(tr_loader,model,criterion,optimizer, num_epochs):\n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "    \n",
        "    \n",
        "    training_log =[]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"started training epoch no. {}\".format(epoch+1))\n",
        "      tr_loss = 0\n",
        "      for step,batch in enumerate(tr_loader):\n",
        "            feats,labels = batch\n",
        "            feats = feats.to(device,dtype=torch.float32)\n",
        "            labels = labels.to(device,dtype=torch.float32)\n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            tr_loss+=loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "      training_log.append({\n",
        "                'epoch':epoch,\n",
        "                'train_loss':tr_loss / len(tr_loader),\n",
        "                })\n",
        "      \n",
        "    return training_log\n",
        "  \n",
        "def test_evaluation(tst_loader,model,criterion): \n",
        "    if torch.has_cuda:\n",
        "      device = torch.device('cuda:0')\n",
        "      model.to(device)\n",
        "    else:\n",
        "      device = torch.device('cpu:0')\n",
        "     \n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "\n",
        "    for step,batch in enumerate(tst_loader):\n",
        "        feats, labels = batch\n",
        "      \n",
        "        feats = feats.to(device,dtype=torch.float32)\n",
        "        labels = labels.to(device,dtype=torch.float32)\n",
        "        outputs = model(feats)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    return test_loss / TEST_SIZE"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFdXs8FUvsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_list = list(range(df_all.shape[0]))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(rand_list)\n",
        "train_idx = rand_list[:int(len(rand_list)*PERC_TRAIN)]\n",
        "test_idx = rand_list[int(len(rand_list)*PERC_TEST):]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0py9jHkqUzdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = DS(df_all.iloc[train_idx,:])\n",
        "test_set = DS(df_all.iloc[test_idx,:])\n",
        "train_loader=DataLoader(dataset= train_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "test_loader=DataLoader(dataset= test_set, batch_size = 4, shuffle = True, num_workers = 2)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OraXdeBuUz6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b905a80-5b80-4cec-f2ee-c78faf7d901e"
      },
      "source": [
        "train_epoch(train_loader,model,criterion,optimizer,num_epochs= 100)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'epoch': 0, 'train_loss': 1.2260447753358859},\n",
              " {'epoch': 1, 'train_loss': 1.2201950534463193},\n",
              " {'epoch': 2, 'train_loss': 1.2139071656850609},\n",
              " {'epoch': 3, 'train_loss': 1.2135979788737994},\n",
              " {'epoch': 4, 'train_loss': 1.2092380053581566},\n",
              " {'epoch': 5, 'train_loss': 1.2106864707545568},\n",
              " {'epoch': 6, 'train_loss': 1.2085914891411371},\n",
              " {'epoch': 7, 'train_loss': 1.2062474128365206},\n",
              " {'epoch': 8, 'train_loss': 1.2080143700340527},\n",
              " {'epoch': 9, 'train_loss': 1.2047713756010825},\n",
              " {'epoch': 10, 'train_loss': 1.2057216532066197},\n",
              " {'epoch': 11, 'train_loss': 1.2047072243759895},\n",
              " {'epoch': 12, 'train_loss': 1.2028801119894752},\n",
              " {'epoch': 13, 'train_loss': 1.2022913705630132},\n",
              " {'epoch': 14, 'train_loss': 1.2007414274511508},\n",
              " {'epoch': 15, 'train_loss': 1.2009135119780028},\n",
              " {'epoch': 16, 'train_loss': 1.1987184838965292},\n",
              " {'epoch': 17, 'train_loss': 1.1972668147654304},\n",
              " {'epoch': 18, 'train_loss': 1.1980588520609503},\n",
              " {'epoch': 19, 'train_loss': 1.1965154926716097},\n",
              " {'epoch': 20, 'train_loss': 1.1987352206498307},\n",
              " {'epoch': 21, 'train_loss': 1.1970228849944042},\n",
              " {'epoch': 22, 'train_loss': 1.196678427149555},\n",
              " {'epoch': 23, 'train_loss': 1.1971885437253154},\n",
              " {'epoch': 24, 'train_loss': 1.1968481778990814},\n",
              " {'epoch': 25, 'train_loss': 1.1959805921231663},\n",
              " {'epoch': 26, 'train_loss': 1.1956273490012206},\n",
              " {'epoch': 27, 'train_loss': 1.1966444064732704},\n",
              " {'epoch': 28, 'train_loss': 1.1953261528022796},\n",
              " {'epoch': 29, 'train_loss': 1.1956673476607942},\n",
              " {'epoch': 30, 'train_loss': 1.1945394193927488},\n",
              " {'epoch': 31, 'train_loss': 1.1943544854638664},\n",
              " {'epoch': 32, 'train_loss': 1.194350804461219},\n",
              " {'epoch': 33, 'train_loss': 1.1937323187733482},\n",
              " {'epoch': 34, 'train_loss': 1.1978135509822196},\n",
              " {'epoch': 35, 'train_loss': 1.1954171355949113},\n",
              " {'epoch': 36, 'train_loss': 1.1968370462679714},\n",
              " {'epoch': 37, 'train_loss': 1.196673536014662},\n",
              " {'epoch': 38, 'train_loss': 1.1976245989093004},\n",
              " {'epoch': 39, 'train_loss': 1.193681262955563},\n",
              " {'epoch': 40, 'train_loss': 1.1949178010937296},\n",
              " {'epoch': 41, 'train_loss': 1.1949949940269047},\n",
              " {'epoch': 42, 'train_loss': 1.196045674041102},\n",
              " {'epoch': 43, 'train_loss': 1.195777617964824},\n",
              " {'epoch': 44, 'train_loss': 1.193968112163337},\n",
              " {'epoch': 45, 'train_loss': 1.195985891910663},\n",
              " {'epoch': 46, 'train_loss': 1.1956021269685029},\n",
              " {'epoch': 47, 'train_loss': 1.1973738936661382},\n",
              " {'epoch': 48, 'train_loss': 1.1952345921102476},\n",
              " {'epoch': 49, 'train_loss': 1.1935779422946398},\n",
              " {'epoch': 50, 'train_loss': 1.1960006655698512},\n",
              " {'epoch': 51, 'train_loss': 1.196489774637542},\n",
              " {'epoch': 52, 'train_loss': 1.1934382836937403},\n",
              " {'epoch': 53, 'train_loss': 1.1955916564152484},\n",
              " {'epoch': 54, 'train_loss': 1.1956870609008508},\n",
              " {'epoch': 55, 'train_loss': 1.1942694096159825},\n",
              " {'epoch': 56, 'train_loss': 1.1952215696321724},\n",
              " {'epoch': 57, 'train_loss': 1.1956953560221844},\n",
              " {'epoch': 58, 'train_loss': 1.195524825224841},\n",
              " {'epoch': 59, 'train_loss': 1.1956162622500988},\n",
              " {'epoch': 60, 'train_loss': 1.1952884865139795},\n",
              " {'epoch': 61, 'train_loss': 1.1933129372677633},\n",
              " {'epoch': 62, 'train_loss': 1.1949227750361193},\n",
              " {'epoch': 63, 'train_loss': 1.1935811926384714},\n",
              " {'epoch': 64, 'train_loss': 1.194793609174182},\n",
              " {'epoch': 65, 'train_loss': 1.1949628855037633},\n",
              " {'epoch': 66, 'train_loss': 1.1970591100254013},\n",
              " {'epoch': 67, 'train_loss': 1.1962899059338445},\n",
              " {'epoch': 68, 'train_loss': 1.1935487783079621},\n",
              " {'epoch': 69, 'train_loss': 1.1947609777193045},\n",
              " {'epoch': 70, 'train_loss': 1.195844805765956},\n",
              " {'epoch': 71, 'train_loss': 1.1949213469777569},\n",
              " {'epoch': 72, 'train_loss': 1.1943737524289535},\n",
              " {'epoch': 73, 'train_loss': 1.194656520740882},\n",
              " {'epoch': 74, 'train_loss': 1.195587493986328},\n",
              " {'epoch': 75, 'train_loss': 1.192819908539518},\n",
              " {'epoch': 76, 'train_loss': 1.1954649968622104},\n",
              " {'epoch': 77, 'train_loss': 1.1956138618677836},\n",
              " {'epoch': 78, 'train_loss': 1.1959075829849213},\n",
              " {'epoch': 79, 'train_loss': 1.194994737066387},\n",
              " {'epoch': 80, 'train_loss': 1.1956336630608897},\n",
              " {'epoch': 81, 'train_loss': 1.1958209188904443},\n",
              " {'epoch': 82, 'train_loss': 1.195353907398181},\n",
              " {'epoch': 83, 'train_loss': 1.1948947180791736},\n",
              " {'epoch': 84, 'train_loss': 1.1960334145647051},\n",
              " {'epoch': 85, 'train_loss': 1.1943136496468256},\n",
              " {'epoch': 86, 'train_loss': 1.1959650155184522},\n",
              " {'epoch': 87, 'train_loss': 1.1957462028108718},\n",
              " {'epoch': 88, 'train_loss': 1.1951829464386277},\n",
              " {'epoch': 89, 'train_loss': 1.1946655251886011},\n",
              " {'epoch': 90, 'train_loss': 1.1952436109176483},\n",
              " {'epoch': 91, 'train_loss': 1.195778272197898},\n",
              " {'epoch': 92, 'train_loss': 1.1960587840175034},\n",
              " {'epoch': 93, 'train_loss': 1.1949699366758013},\n",
              " {'epoch': 94, 'train_loss': 1.1954884797340655},\n",
              " {'epoch': 95, 'train_loss': 1.1953505900185692},\n",
              " {'epoch': 96, 'train_loss': 1.1954669245357759},\n",
              " {'epoch': 97, 'train_loss': 1.1956032731343502},\n",
              " {'epoch': 98, 'train_loss': 1.1958079777061161},\n",
              " {'epoch': 99, 'train_loss': 1.1931690569982738}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruCoMg8JYvOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "3b6d4f42-cfa5-4977-c9c6-dcf5927c8f83"
      },
      "source": [
        "all_dataset_loss = test_evaluation(test_loader,model,criterion)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e3bb4855157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_dataset_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_evaluation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbMEzDjzQoIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "370f32f6-f9e5-4641-fd24-d01e8ca5da55"
      },
      "source": [
        "all_dataset_loss"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1900177375185181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR30VFS0WWiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = np.array(df_all.iloc[:,:-1])\n",
        "labels = np.array(df_all.iloc[:,-1])"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86cm4C6U1dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.has_cuda:\n",
        "  device = torch.device('cuda:0')\n",
        "  model.to(device)\n",
        "else:\n",
        "  device = torch.device('cpu:0')\n",
        "\n",
        "predictions = model(torch.tensor(features, dtype=torch.float32).to(device))\n",
        "predictions  = predictions.cpu().detach().numpy()"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8aOzM-cWdow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred = df.copy()\n",
        "df_pred['Predictions_MLP'] = pd.Series(predictions.reshape(-1))\n",
        "df_pred.to_csv(\"combined_data_with_predictions_both.csv\")"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD04MjKAWw3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9a4ddf3e-c818-4450-b508-3c573ebc7e8b"
      },
      "source": [
        "!git add ./combined_data_with_predictions_both.csv\n",
        "!git config --global user.email \"shaulsolomon@gmail.com\"\n",
        "!git config --global user.name \"Shaul Solomon\"\n",
        "!git commit -m \"Added MLP pred to whole dataset with linear pred\"\n",
        "!git push"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master d6f0e3e] Added MLP pred to whole dataset with linear pred\n",
            " 1 file changed, 24902 insertions(+), 24902 deletions(-)\n",
            "To https://github.com/Hernanros/SOTA\n",
            " ! [rejected]        master -> master (fetch first)\n",
            "error: failed to push some refs to 'https://ShaulSolomon:iamaHippo1492@github.com/Hernanros/SOTA'\n",
            "hint: Updates were rejected because the remote contains work that you do\n",
            "hint: not have locally. This is usually caused by another repository pushing\n",
            "hint: to the same ref. You may want to first integrate the remote changes\n",
            "hint: (e.g., 'git pull ...') before pushing again.\n",
            "hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBhBDQJHW2VK",
        "colab_type": "text"
      },
      "source": [
        "# For each dataset seperately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hcx4CK7X-6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_each = df.drop(columns=[\"text_1\",\"text_2\",\"text_1_tokens\",\"text_2_tokens\",\"Predictions\"]).groupby(\"dataset\")"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAmC0zYXAnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "484b8903-9dbe-41fe-b48c-02dea38fec7f"
      },
      "source": [
        "log = {}\n",
        "all_pred = {}\n",
        "\n",
        "for name, df_group_each in df_each:\n",
        "\n",
        "  df_group = df_group_each.copy()\n",
        "  df_group.drop(columns=\"dataset\",inplace=True)\n",
        "  cols = df_group.columns.to_list()\n",
        "  cols.remove(\"label\")\n",
        "  cols.append(\"label\")\n",
        "  df_group = df_group[cols]\n",
        "\n",
        "  num_features = df_group.shape[1] - 1\n",
        "  num_hl = 128\n",
        "  num_output = 1\n",
        "\n",
        "  DATA_SIZE = df_group.shape[0]\n",
        "  PERC_TRAIN = 0.8\n",
        "  PERC_TEST = 1 - PERC_TRAIN\n",
        "  TRAIN_SIZE = int(DATA_SIZE*PERC_TRAIN)\n",
        "  TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
        "\n",
        "  model = Basemodel(num_features,num_hl,num_output)\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  rand_list = list(range(df_group.shape[0]))\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(rand_list)\n",
        "  train_idx = rand_list[:int(len(rand_list)*PERC_TRAIN)]\n",
        "  test_idx = rand_list[int(len(rand_list)*PERC_TEST):]\n",
        "  train_set = DS(df_group.iloc[train_idx,:])\n",
        "  test_set = DS(df_group.iloc[test_idx,:])\n",
        "\n",
        "  train_loader=DataLoader(dataset= train_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "  test_loader=DataLoader(dataset= test_set, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "\n",
        "  train_epoch(train_loader,model,criterion,optimizer,num_epochs= 100)\n",
        "  loss = test_evaluation(test_loader,model,criterion)\n",
        "  log[name] = loss\n",
        "\n",
        "  features = np.array(df_all.iloc[:,:-1])\n",
        "  labels = np.array(df_all.iloc[:,-1])\n",
        "\n",
        "  if torch.has_cuda:\n",
        "    device = torch.device('cuda:0')\n",
        "    model.to(device)\n",
        "  else:\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "  predictions = model(torch.tensor(features, dtype=torch.float32).to(device))\n",
        "  predictions  = predictions.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  all_pred[name] = pd.Series(predictions.reshape(-1))\n",
        "\n",
        "print(log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n",
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n",
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n",
            "started training epoch no. 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n",
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n",
            "started training epoch no. 29\n",
            "started training epoch no. 30\n",
            "started training epoch no. 31\n",
            "started training epoch no. 32\n",
            "started training epoch no. 33\n",
            "started training epoch no. 34\n",
            "started training epoch no. 35\n",
            "started training epoch no. 36\n",
            "started training epoch no. 37\n",
            "started training epoch no. 38\n",
            "started training epoch no. 39\n",
            "started training epoch no. 40\n",
            "started training epoch no. 41\n",
            "started training epoch no. 42\n",
            "started training epoch no. 43\n",
            "started training epoch no. 44\n",
            "started training epoch no. 45\n",
            "started training epoch no. 46\n",
            "started training epoch no. 47\n",
            "started training epoch no. 48\n",
            "started training epoch no. 49\n",
            "started training epoch no. 50\n",
            "started training epoch no. 51\n",
            "started training epoch no. 52\n",
            "started training epoch no. 53\n",
            "started training epoch no. 54\n",
            "started training epoch no. 55\n",
            "started training epoch no. 56\n",
            "started training epoch no. 57\n",
            "started training epoch no. 58\n",
            "started training epoch no. 59\n",
            "started training epoch no. 60\n",
            "started training epoch no. 61\n",
            "started training epoch no. 62\n",
            "started training epoch no. 63\n",
            "started training epoch no. 64\n",
            "started training epoch no. 65\n",
            "started training epoch no. 66\n",
            "started training epoch no. 67\n",
            "started training epoch no. 68\n",
            "started training epoch no. 69\n",
            "started training epoch no. 70\n",
            "started training epoch no. 71\n",
            "started training epoch no. 72\n",
            "started training epoch no. 73\n",
            "started training epoch no. 74\n",
            "started training epoch no. 75\n",
            "started training epoch no. 76\n",
            "started training epoch no. 77\n",
            "started training epoch no. 78\n",
            "started training epoch no. 79\n",
            "started training epoch no. 80\n",
            "started training epoch no. 81\n",
            "started training epoch no. 82\n",
            "started training epoch no. 83\n",
            "started training epoch no. 84\n",
            "started training epoch no. 85\n",
            "started training epoch no. 86\n",
            "started training epoch no. 87\n",
            "started training epoch no. 88\n",
            "started training epoch no. 89\n",
            "started training epoch no. 90\n",
            "started training epoch no. 91\n",
            "started training epoch no. 92\n",
            "started training epoch no. 93\n",
            "started training epoch no. 94\n",
            "started training epoch no. 95\n",
            "started training epoch no. 96\n",
            "started training epoch no. 97\n",
            "started training epoch no. 98\n",
            "started training epoch no. 99\n",
            "started training epoch no. 100\n",
            "started training epoch no. 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "started training epoch no. 2\n",
            "started training epoch no. 3\n",
            "started training epoch no. 4\n",
            "started training epoch no. 5\n",
            "started training epoch no. 6\n",
            "started training epoch no. 7\n",
            "started training epoch no. 8\n",
            "started training epoch no. 9\n",
            "started training epoch no. 10\n",
            "started training epoch no. 11\n",
            "started training epoch no. 12\n",
            "started training epoch no. 13\n",
            "started training epoch no. 14\n",
            "started training epoch no. 15\n",
            "started training epoch no. 16\n",
            "started training epoch no. 17\n",
            "started training epoch no. 18\n",
            "started training epoch no. 19\n",
            "started training epoch no. 20\n",
            "started training epoch no. 21\n",
            "started training epoch no. 22\n",
            "started training epoch no. 23\n",
            "started training epoch no. 24\n",
            "started training epoch no. 25\n",
            "started training epoch no. 26\n",
            "started training epoch no. 27\n",
            "started training epoch no. 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSGaTt_Jzwq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "df_pred = df.copy()\n",
        "for name, group in df_pred.groupby(\"dataset\"):\n",
        "  group['Predictions_MLP'] = all_pred[name]\n",
        "  frames.append(group)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPBPl4Jp39Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_prediction_result = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrT9guiR3Iqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_prediction_result.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQBT7C7_50DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_prediction_result.to_csv('combined_data_with_predictions_on_separate_datasets_both.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vw-QuhH6QMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shouldve turned it into a variable before, but saving it now\n",
        "log['all_dataset_loss'] = all_dataset_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm-nNLCw6dhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame.from_dict(log,orient=\"index\").to_csv('test_loss_on_MLP.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXpfclYr7C5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add ./combined_data_with_predictions_on_separate_datasets_both.csv\n",
        "!git add ./test_loss_on_MLP.csv\n",
        "!git config --global user.email \"shaulsolomon@gmail.com\"\n",
        "!git config --global user.name \"Shaul Solomon\"\n",
        "!git commit -m \"Added MLP pred for all datasets\"\n",
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bShfBViL7Slf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}