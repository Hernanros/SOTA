{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lee_mlp_model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tTgEUCR6n3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd   \n",
        "import re\n",
        "import os   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNVCA51663E1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "745c0f50-1b0e-4808-db7d-2250cfaea1b2"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWXwVcse63jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "65ff4532-18fa-4181-d66d-6725cfd04c38"
      },
      "source": [
        "cd \"/content/drive/My Drive/Data Science Course/semester 2/NLP/homework/HW3 - semantic similarity\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Data Science Course/semester 2/NLP/homework/HW3 - semantic similarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSYtO4Fz7ITi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"labeled_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMGSYNyz7Tkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "f4fddeec-789e-48b0-d449-1daab76fa2e6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>POS dist score</th>\n",
              "      <th>1-gram_overlap</th>\n",
              "      <th>chrf_score_norm</th>\n",
              "      <th>WMD</th>\n",
              "      <th>ROUGE-1 recall</th>\n",
              "      <th>ROUGE-1 precision</th>\n",
              "      <th>ROUGE-1 F</th>\n",
              "      <th>ROUGE-2 recall</th>\n",
              "      <th>ROUGE-2 precision</th>\n",
              "      <th>ROUGE-2 F</th>\n",
              "      <th>ROUGE-L recall</th>\n",
              "      <th>ROUGE-L precision</th>\n",
              "      <th>ROUGE-L F</th>\n",
              "      <th>BertScore</th>\n",
              "      <th>L2_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>take measures in order to</td>\n",
              "      <td>take additional measures to</td>\n",
              "      <td>4.000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.533040</td>\n",
              "      <td>1.382194</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>4.5364</td>\n",
              "      <td>2.621138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>was seeking to</td>\n",
              "      <td>will strive to</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>3.197893</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>4.5531</td>\n",
              "      <td>2.147480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>canada aims to</td>\n",
              "      <td>canada works to</td>\n",
              "      <td>2.333</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.658046</td>\n",
              "      <td>2.067205</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>4.7772</td>\n",
              "      <td>2.539115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>refugees and</td>\n",
              "      <td>refugee population</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.305241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.475248</td>\n",
              "      <td>2.630209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.3771</td>\n",
              "      <td>0.842952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>measures and</td>\n",
              "      <td>instruments , and</td>\n",
              "      <td>3.000</td>\n",
              "      <td>2.688024</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2.365262</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.4446</td>\n",
              "      <td>1.865555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                     text_1  ... BertScore  L2_score\n",
              "0           0  take measures in order to  ...    4.5364  2.621138\n",
              "1           1             was seeking to  ...    4.5531  2.147480\n",
              "2           2             canada aims to  ...    4.7772  2.539115\n",
              "3           3               refugees and  ...    4.3771  0.842952\n",
              "4           4               measures and  ...    4.4446  1.865555\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvXGCoX7gQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(columns=[\"Unnamed: 0\",\"text_1\",\"text_2\"],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qSgIg67saS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "59a674d0-4f4c-40ee-8f3e-384e0dae1ef2"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>POS dist score</th>\n",
              "      <th>1-gram_overlap</th>\n",
              "      <th>chrf_score_norm</th>\n",
              "      <th>WMD</th>\n",
              "      <th>ROUGE-1 recall</th>\n",
              "      <th>ROUGE-1 precision</th>\n",
              "      <th>ROUGE-1 F</th>\n",
              "      <th>ROUGE-2 recall</th>\n",
              "      <th>ROUGE-2 precision</th>\n",
              "      <th>ROUGE-2 F</th>\n",
              "      <th>ROUGE-L recall</th>\n",
              "      <th>ROUGE-L precision</th>\n",
              "      <th>ROUGE-L F</th>\n",
              "      <th>BertScore</th>\n",
              "      <th>L2_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.533040</td>\n",
              "      <td>1.382194</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>4.5364</td>\n",
              "      <td>2.621138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>3.197893</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>4.5531</td>\n",
              "      <td>2.147480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.333</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.658046</td>\n",
              "      <td>2.067205</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>4.7772</td>\n",
              "      <td>2.539115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.667</td>\n",
              "      <td>2.305241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.475248</td>\n",
              "      <td>2.630209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.3771</td>\n",
              "      <td>0.842952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.000</td>\n",
              "      <td>2.688024</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2.365262</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.4446</td>\n",
              "      <td>1.865555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  POS dist score  1-gram_overlap  ...  ROUGE-L F  BertScore  L2_score\n",
              "0  4.000        5.000000        3.333333  ...   3.333333     4.5364  2.621138\n",
              "1  3.000        0.000000        1.666667  ...   1.666667     4.5531  2.147480\n",
              "2  2.333        5.000000        3.333333  ...   3.333333     4.7772  2.539115\n",
              "3  2.667        2.305241        0.000000  ...   0.000000     4.3771  0.842952\n",
              "4  3.000        2.688024        2.500000  ...   2.000000     4.4446  1.865555\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdExXHza7tKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)    # reproducible\n",
        "x = torch.tensor(np.array(data.iloc[:,1:]), dtype=torch.float32) #take only features\n",
        "y = torch.tensor(np.array(data.iloc[:,0]), dtype=torch.float32) #take only lables "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gvoONJzSK8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34d80f73-2820-4ea6-f5f9-0fe021cb3654"
      },
      "source": [
        "x.shape , y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([998, 15]), torch.Size([998]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hy4D7TBKH57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(x.shape[1], 50),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(50, 100),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(100, 1),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHTjOJuSQD9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNlepO3tQEYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCH = 200\n",
        "\n",
        "\n",
        "train_torch_dataset = Data.TensorDataset(x[:750, :], y[:750])\n",
        "test_torch_dataset = Data.TensorDataset(x[750:, :], y[750:])\n",
        "\n",
        "train_loader = Data.DataLoader(dataset=train_torch_dataset, batch_size=BATCH_SIZE,  shuffle=True, num_workers=2,)\n",
        "test_loader = Data.DataLoader(dataset=test_torch_dataset, batch_size=BATCH_SIZE,  shuffle=True, num_workers=2,)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjIlcstX2Eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d5ba65e-c34a-4ac9-dd5c-3bb7d56d4b7c"
      },
      "source": [
        "# start training\n",
        "for epoch in range(EPOCH):\n",
        "  training_loss = 0\n",
        "  for step, (batch_x, batch_y) in enumerate(train_loader): # for each training step\n",
        "          \n",
        "        \n",
        "    b_x = Variable(batch_x)\n",
        "    b_y = Variable(batch_y)\n",
        "\n",
        "    prediction = net(b_x)     # input x and predict based on x\n",
        "    loss = loss_func(prediction, b_y)     # must be (1. nn output, 2. target)\n",
        "\n",
        "    optimizer.zero_grad()   # clear gradients for next train\n",
        "    loss.backward()         # backpropagation, compute gradients\n",
        "    optimizer.step()        # apply gradients\n",
        "\n",
        "    training_loss += loss.item()\n",
        "  print(f\"Training loss: {training_loss/len(train_loader)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss: 1.0031059851196218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.9052119236835774\n",
            "Training loss: 0.7653577500518333\n",
            "Training loss: 0.720690657563032\n",
            "Training loss: 0.7383819633222958\n",
            "Training loss: 0.7095212718779023\n",
            "Training loss: 0.7066368588464375\n",
            "Training loss: 0.711865505381943\n",
            "Training loss: 0.7247187855078819\n",
            "Training loss: 0.6951543541347727\n",
            "Training loss: 0.6902094496057388\n",
            "Training loss: 0.6630028595236388\n",
            "Training loss: 0.6666087029164934\n",
            "Training loss: 0.668081790446601\n",
            "Training loss: 0.6789241457863294\n",
            "Training loss: 0.6547832225568275\n",
            "Training loss: 0.643936472984546\n",
            "Training loss: 0.6704676766146688\n",
            "Training loss: 0.64502935936833\n",
            "Training loss: 0.6825262764270635\n",
            "Training loss: 0.6476007602474791\n",
            "Training loss: 0.65065021876325\n",
            "Training loss: 0.6444449744960095\n",
            "Training loss: 0.6800768067663971\n",
            "Training loss: 0.6535788015165227\n",
            "Training loss: 0.6801383372079185\n",
            "Training loss: 0.640599477379781\n",
            "Training loss: 0.6561574806716848\n",
            "Training loss: 0.6842093424217657\n",
            "Training loss: 0.6547619339197557\n",
            "Training loss: 0.6451002241686937\n",
            "Training loss: 0.6446619943487755\n",
            "Training loss: 0.657019596685596\n",
            "Training loss: 0.6429762919730646\n",
            "Training loss: 0.6542140380578473\n",
            "Training loss: 0.6693633568492976\n",
            "Training loss: 0.6614032468679262\n",
            "Training loss: 0.6569434157176379\n",
            "Training loss: 0.6617968810048509\n",
            "Training loss: 0.6536228538867324\n",
            "Training loss: 0.6553397630440428\n",
            "Training loss: 0.6364234109072292\n",
            "Training loss: 0.6470445354092629\n",
            "Training loss: 0.6492952564037702\n",
            "Training loss: 0.642126584821876\n",
            "Training loss: 0.6805232845573745\n",
            "Training loss: 0.6608496329331017\n",
            "Training loss: 0.6452077817409596\n",
            "Training loss: 0.6462752343467536\n",
            "Training loss: 0.6555023565888405\n",
            "Training loss: 0.6442696497319861\n",
            "Training loss: 0.6737931281367832\n",
            "Training loss: 0.6605716728545884\n",
            "Training loss: 0.6544240591453111\n",
            "Training loss: 0.6448690535579907\n",
            "Training loss: 0.6520991243838155\n",
            "Training loss: 0.6444336021953123\n",
            "Training loss: 0.6447083341710745\n",
            "Training loss: 0.6405999018077521\n",
            "Training loss: 0.6386626457121778\n",
            "Training loss: 0.653453529515165\n",
            "Training loss: 0.700960374551211\n",
            "Training loss: 0.6593248923110994\n",
            "Training loss: 0.6637813044474163\n",
            "Training loss: 0.6442125145385557\n",
            "Training loss: 0.6432257662031878\n",
            "Training loss: 0.6470221382308197\n",
            "Training loss: 0.6423338447162446\n",
            "Training loss: 0.6428973210103651\n",
            "Training loss: 0.6749258936045969\n",
            "Training loss: 0.6435179073918373\n",
            "Training loss: 0.6444591694075535\n",
            "Training loss: 0.6454989296542679\n",
            "Training loss: 0.6800717874331043\n",
            "Training loss: 0.6415815483382408\n",
            "Training loss: 0.6444398118579324\n",
            "Training loss: 0.643479553110739\n",
            "Training loss: 0.6430450119177237\n",
            "Training loss: 0.6423485186087226\n",
            "Training loss: 0.6580120906372495\n",
            "Training loss: 0.642388459527865\n",
            "Training loss: 0.651731239593999\n",
            "Training loss: 0.6626695022284192\n",
            "Training loss: 0.6457224581390619\n",
            "Training loss: 0.6536324764106501\n",
            "Training loss: 0.6567529281403156\n",
            "Training loss: 0.6417023895625421\n",
            "Training loss: 0.6508820657717421\n",
            "Training loss: 0.6425625399408981\n",
            "Training loss: 0.6414775021394041\n",
            "Training loss: 0.6609915061457519\n",
            "Training loss: 0.6437350559107801\n",
            "Training loss: 0.63879932497846\n",
            "Training loss: 0.6403618805665285\n",
            "Training loss: 0.6389846243360575\n",
            "Training loss: 0.6378490347217055\n",
            "Training loss: 0.6350510367093251\n",
            "Training loss: 3.422884510780506\n",
            "Training loss: 0.696356364258347\n",
            "Training loss: 0.6482898569764927\n",
            "Training loss: 0.6410338918435724\n",
            "Training loss: 0.6427514170237044\n",
            "Training loss: 0.6415559849008283\n",
            "Training loss: 0.641849000611283\n",
            "Training loss: 0.6395870631739021\n",
            "Training loss: 0.6404029159271654\n",
            "Training loss: 0.6480872933851912\n",
            "Training loss: 0.6358475418008388\n",
            "Training loss: 0.6383398398558827\n",
            "Training loss: 0.6507381863812817\n",
            "Training loss: 0.637563597074056\n",
            "Training loss: 0.6364263594868851\n",
            "Training loss: 0.6362692246689124\n",
            "Training loss: 0.6385503740228237\n",
            "Training loss: 0.6364042165273047\n",
            "Training loss: 0.643171402863524\n",
            "Training loss: 0.6377954561798338\n",
            "Training loss: 0.6335357787345159\n",
            "Training loss: 0.6368018984794617\n",
            "Training loss: 0.6392483607410117\n",
            "Training loss: 0.6365191316073562\n",
            "Training loss: 0.6402874082683566\n",
            "Training loss: 0.6399106274933574\n",
            "Training loss: 0.6372638570263665\n",
            "Training loss: 0.6518987384723857\n",
            "Training loss: 0.6349662526986225\n",
            "Training loss: 0.6385386111511988\n",
            "Training loss: 0.6322698100331299\n",
            "Training loss: 0.6393024002578347\n",
            "Training loss: 0.6373186783983986\n",
            "Training loss: 0.6385032933086474\n",
            "Training loss: 0.635143752625965\n",
            "Training loss: 0.6316170798654251\n",
            "Training loss: 0.6365602889217119\n",
            "Training loss: 0.6427473934773514\n",
            "Training loss: 0.634533055191067\n",
            "Training loss: 0.6417300790409617\n",
            "Training loss: 0.648345526823021\n",
            "Training loss: 0.6805589641345308\n",
            "Training loss: 0.6348389407342419\n",
            "Training loss: 0.6347864664932514\n",
            "Training loss: 0.6350738850838327\n",
            "Training loss: 0.6406931134257862\n",
            "Training loss: 0.6343682354236854\n",
            "Training loss: 0.6393647427135642\n",
            "Training loss: 0.6411411084452684\n",
            "Training loss: 0.692500004009839\n",
            "Training loss: 0.643296694769425\n",
            "Training loss: 0.6407651551701921\n",
            "Training loss: 0.643884446016176\n",
            "Training loss: 0.6421912816808896\n",
            "Training loss: 0.6447660402454277\n",
            "Training loss: 0.634085495232981\n",
            "Training loss: 0.6328385308226372\n",
            "Training loss: 0.6334547992359768\n",
            "Training loss: 0.633200075814223\n",
            "Training loss: 0.6421195683168605\n",
            "Training loss: 0.6343408318315732\n",
            "Training loss: 0.6365524174922959\n",
            "Training loss: 0.6358361955424969\n",
            "Training loss: 0.6387987582290426\n",
            "Training loss: 0.6329701771721878\n",
            "Training loss: 0.6330196635123897\n",
            "Training loss: 0.6378709118258445\n",
            "Training loss: 0.6338396004222809\n",
            "Training loss: 0.6351007739517917\n",
            "Training loss: 0.6341544436489014\n",
            "Training loss: 0.713690667730221\n",
            "Training loss: 0.6411478084294085\n",
            "Training loss: 0.6384135737539606\n",
            "Training loss: 0.6359745344788609\n",
            "Training loss: 0.6365493613473595\n",
            "Training loss: 0.6448105329330615\n",
            "Training loss: 0.6473447339252588\n",
            "Training loss: 0.6344459539754911\n",
            "Training loss: 0.6346591761771668\n",
            "Training loss: 0.6315014056782139\n",
            "Training loss: 0.6312645317075101\n",
            "Training loss: 0.633228325047233\n",
            "Training loss: 0.6333082031379355\n",
            "Training loss: 0.6337048901601675\n",
            "Training loss: 0.6375799761649142\n",
            "Training loss: 0.728363147094608\n",
            "Training loss: 0.6420472958224251\n",
            "Training loss: 0.637713519024088\n",
            "Training loss: 0.641468927125506\n",
            "Training loss: 0.6419949942327877\n",
            "Training loss: 0.6447768366320971\n",
            "Training loss: 0.6339964531362057\n",
            "Training loss: 0.6374428405604781\n",
            "Training loss: 0.6444860625060949\n",
            "Training loss: 0.6397307061570439\n",
            "Training loss: 0.6422344649785218\n",
            "Training loss: 0.6335895803578674\n",
            "Training loss: 0.6369836675399478\n",
            "Training loss: 0.6378277122201597\n",
            "Training loss: 0.6380160020149135\n",
            "Training loss: 0.6344487246007045\n",
            "Training loss: 0.6490460716663523\n",
            "Training loss: 0.6436435413903537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9kA-qg7kBG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "4a660520-5da4-4e53-d439-e5fe3a2b5719"
      },
      "source": [
        "# start testing\n",
        "\n",
        "testing_loss = 0\n",
        "for step, (batch_x, batch_y) in enumerate(test_loader): # for each training step\n",
        "        \n",
        "  b_x = Variable(batch_x)\n",
        "  b_y = Variable(batch_y)\n",
        "\n",
        "  prediction = net(b_x)     # input x and predict based on x\n",
        "  loss = loss_func(prediction, b_y)     # must be (1. nn output, 2. target)\n",
        "\n",
        "  testing_loss += loss.item()\n",
        "print(f\"testing loss: {testing_loss/len(test_loader)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.6339470339157889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3FOSmV_oL03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2569ed13-9b55-4f4d-ab95-cc97629476ae"
      },
      "source": [
        "\n",
        "w = list(net.parameters())\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 1.6080e-01, -4.7701e-01, -1.1910e+00,  3.7882e-01, -2.6354e-03,\n",
              "          -7.6638e-02, -1.4267e-01,  6.8461e-01,  3.2823e-01,  2.2673e-01,\n",
              "           1.0925e-02, -5.2119e-01, -2.9785e-01, -8.9852e-01, -1.1012e+00],\n",
              "         [-1.1339e+00, -1.2170e-01, -7.5101e-01, -1.2713e-01,  2.4377e-01,\n",
              "          -2.3888e-01, -1.7110e-01, -3.8567e-01, -7.3630e-01, -6.1386e-01,\n",
              "           8.9916e-02, -6.5075e-02,  4.1878e-02, -8.5701e-01, -5.6861e-01],\n",
              "         [-2.3527e-01,  2.9357e-02, -6.2764e-01,  3.6230e-01, -9.5669e-02,\n",
              "          -3.1904e-01,  2.1574e-01, -4.8795e-02, -2.2181e-01, -3.7442e-01,\n",
              "           2.6746e-01, -4.4817e-01,  3.9956e-02, -5.0411e-01, -5.1923e-01],\n",
              "         [-3.7311e-01, -2.3439e-01, -1.6399e+00,  2.5981e-01,  7.4721e-02,\n",
              "          -9.5946e-02, -2.9540e-02, -3.3854e-02,  2.6345e-01,  3.6847e-01,\n",
              "           2.0863e-01,  4.8608e-02, -1.7520e-01, -9.7328e-01, -1.2328e+00],\n",
              "         [-3.5793e-01, -1.7657e-01, -7.6195e-01, -3.6418e-01, -2.7561e-01,\n",
              "          -8.1255e-02, -1.1457e-01,  3.0267e-01,  1.7241e-01,  1.3706e-01,\n",
              "          -3.5185e-02, -1.4105e-01, -2.6749e-02, -6.9919e-01, -6.7252e-01],\n",
              "         [ 5.5714e-02, -1.7694e-01, -8.8868e-01, -9.7198e-02,  3.7384e-01,\n",
              "          -2.8970e-01, -1.5895e-01,  6.0742e-02, -3.3018e-01, -3.8827e-02,\n",
              "          -3.5266e-02, -4.7269e-01,  9.4689e-02, -7.0852e-01, -2.4678e-01],\n",
              "         [ 3.5770e-01, -4.7538e-01, -9.2766e-01, -2.3847e-01, -4.5348e-03,\n",
              "          -3.6074e-01, -2.9110e-02,  8.5592e-02, -5.7698e-01, -4.4207e-01,\n",
              "           2.6061e-01, -4.2877e-01, -1.1382e-01, -5.2981e-01, -5.4970e-01],\n",
              "         [ 8.9982e-02, -4.6809e-02, -4.4413e-01,  2.5067e-03,  9.0156e-02,\n",
              "          -5.7931e-02,  1.2840e-01,  2.8126e-01, -4.3314e-01, -1.2809e-01,\n",
              "           1.7230e-01, -1.2418e-02,  3.6374e-02, -4.2816e-01, -2.9245e-01],\n",
              "         [-3.1076e-01, -4.0213e-02, -3.0625e-01, -6.1701e-01,  2.1998e-01,\n",
              "          -4.5340e-02, -1.4302e-01, -3.9169e-02,  1.4611e-01,  3.5053e-01,\n",
              "          -9.3340e-02,  1.5172e-01,  3.1500e-01, -2.5684e-01, -2.8175e-01],\n",
              "         [-3.4603e-02,  1.2864e-01, -6.7875e-01, -4.6404e-01,  3.1646e-02,\n",
              "          -2.4888e-01,  2.8960e-02, -3.4269e-01, -2.4157e-01, -1.8609e-01,\n",
              "           1.6599e-01, -6.1054e-02,  1.1617e-01, -5.5788e-01, -5.4315e-01],\n",
              "         [-2.6240e-01, -1.1143e+00, -1.4008e+00, -4.2325e-02, -1.3136e+00,\n",
              "          -1.4152e+00, -1.3435e+00, -1.1282e+00, -9.6803e-01, -1.2438e+00,\n",
              "          -1.3517e+00, -1.2409e+00, -1.2016e+00, -8.0532e-01, -1.0973e+00],\n",
              "         [-6.3814e-01, -2.6736e-01, -1.1142e+00,  5.1602e-01,  2.4041e-01,\n",
              "          -5.3874e-01,  1.6936e-03,  3.6997e-02, -4.4345e-01, -8.9422e-02,\n",
              "           4.6983e-01, -4.1549e-01, -2.3554e-01, -1.0128e+00, -9.3580e-01],\n",
              "         [-8.8380e-01, -2.5893e-01, -1.0581e+00, -4.3448e-01, -1.3547e-01,\n",
              "           4.6382e-02, -3.1816e-01, -3.9476e-01, -3.5800e-01, -4.2356e-01,\n",
              "           5.2512e-02, -3.3528e-01, -2.7354e-01, -5.8010e-01, -3.4630e-01],\n",
              "         [ 7.3027e-01,  5.1470e-02, -1.3919e+00,  1.3748e-01,  3.7134e-01,\n",
              "          -6.7534e-02, -3.8328e-02,  4.6040e-01, -5.6559e-01, -7.5356e-02,\n",
              "           3.5826e-01, -3.5308e-01, -5.7734e-02, -7.6835e-01, -1.0805e+00],\n",
              "         [ 2.6757e-01, -2.2133e-01, -1.2312e+00,  4.6822e-01,  4.4557e-01,\n",
              "           3.7183e-02,  1.3306e-01, -2.1567e-02, -2.2422e-01,  1.0296e-01,\n",
              "          -7.5173e-02, -3.2706e-02, -3.5564e-01, -1.0494e+00, -8.6540e-01],\n",
              "         [-1.5654e-01, -1.3958e-01,  3.2398e-02, -1.0006e-01,  7.0712e-01,\n",
              "          -1.5769e-01, -1.3728e-01, -1.2561e-01, -1.2353e-01, -2.6710e-01,\n",
              "           5.1200e-01, -3.0642e-01,  2.1573e-01, -3.8753e-01, -6.4515e-01],\n",
              "         [ 4.4117e-01, -1.2353e-01, -1.5547e+00,  3.4476e-01,  2.3559e-01,\n",
              "          -2.8119e-01, -2.7663e-01,  7.3757e-01,  1.9485e-01,  6.9289e-01,\n",
              "           5.0324e-01, -5.1126e-01, -1.4423e-01, -6.9955e-01, -1.0542e+00],\n",
              "         [-3.4582e-01, -4.2744e-01, -8.0941e-01, -1.8099e-01, -3.1173e-01,\n",
              "          -2.4841e-01, -1.7844e-01, -6.2275e-01, -1.7863e-01, -4.4001e-01,\n",
              "          -5.1787e-01, -4.0971e-01, -6.6540e-01, -4.8946e-01, -8.3480e-01],\n",
              "         [-3.8379e-01, -3.0552e-01, -8.2250e-01, -1.9945e-01, -4.9965e-02,\n",
              "          -3.3494e-01, -3.1441e-01,  4.6990e-01, -7.3607e-05,  3.1155e-01,\n",
              "          -1.5737e-01, -2.9089e-01, -2.5357e-02, -5.8483e-01, -5.9078e-01],\n",
              "         [-6.2537e-01, -5.5105e-01, -8.5169e-01,  6.5175e-01,  1.6137e-02,\n",
              "          -3.5708e-01, -3.5139e-01, -5.6346e-01, -3.9152e-01, -6.0132e-01,\n",
              "          -2.5008e-01, -5.3926e-01, -2.4847e-01, -6.4181e-01, -7.1242e-01],\n",
              "         [-5.5261e-02, -9.3474e-01, -1.9334e+00,  1.0714e+00,  5.6620e-02,\n",
              "          -1.1418e+00, -8.5039e-01,  4.7520e-01, -1.1029e+00, -6.4421e-01,\n",
              "          -3.0979e-01, -1.4643e+00, -1.2180e+00, -1.3544e+00, -1.3515e+00],\n",
              "         [-6.0273e-01, -2.8107e-01, -1.0448e+00,  3.6862e-01,  2.3691e-01,\n",
              "          -3.8470e-01, -1.5118e-01, -2.2912e-01, -6.7316e-01, -3.6750e-01,\n",
              "          -2.9078e-02, -5.8989e-01, -1.7257e-01, -8.3200e-01, -5.3899e-01],\n",
              "         [-1.9427e-02, -1.1213e-01, -4.7661e-01, -8.4578e-01, -4.3447e-02,\n",
              "          -8.4524e-02, -1.6866e-01, -8.1856e-01, -6.8240e-01, -7.3316e-01,\n",
              "          -1.8231e-01, -2.0392e-01, -2.4811e-01, -1.6717e-01, -5.2461e-01],\n",
              "         [ 2.8450e-02, -3.2553e-01, -5.5800e-01, -1.7448e-01, -2.2453e-01,\n",
              "          -1.1772e-01, -1.3985e-01, -3.3425e-01, -4.7548e-01, -1.7629e-01,\n",
              "           6.7961e-02, -7.4095e-02, -1.6728e-01, -3.6018e-01, -4.0068e-01],\n",
              "         [ 2.1520e-01, -8.2232e-01, -9.2217e-01,  4.1003e-01, -5.0823e-02,\n",
              "          -6.1987e-01, -6.8105e-01, -1.0741e-01, -3.5566e-01, -3.6998e-01,\n",
              "          -5.2137e-01, -6.5521e-01, -5.4413e-01, -9.0036e-01, -9.5640e-01],\n",
              "         [-8.9493e-01, -1.9913e-01, -6.5909e-01, -2.2397e-01,  3.1595e-01,\n",
              "          -2.8345e-02, -6.4935e-02, -2.8850e-01, -8.1919e-01, -8.6805e-01,\n",
              "           3.3988e-01,  6.9819e-03,  1.1041e-01, -6.6253e-01, -5.5817e-01],\n",
              "         [-2.5169e-01, -2.9493e-01, -1.0375e+00, -2.4701e-01,  3.1599e-01,\n",
              "          -4.8711e-01, -8.7802e-02,  9.7531e-01,  8.5701e-02,  3.6209e-01,\n",
              "          -2.2561e-01, -4.5047e-01, -4.3691e-01, -1.0462e+00, -5.8032e-01],\n",
              "         [ 2.1586e-02, -5.8688e-01, -7.8982e-01, -7.9690e-01, -4.0544e-01,\n",
              "          -2.5066e-01, -1.7932e-01,  2.9271e-02,  5.3088e-01,  5.0646e-01,\n",
              "          -2.9115e-01, -4.6738e-01, -1.3056e-01, -1.0251e+00, -5.6702e-01],\n",
              "         [-5.6184e-01,  2.6206e-03, -1.0452e+00,  3.7767e-01,  3.4512e-02,\n",
              "           5.6537e-02, -1.3525e-01, -3.6690e-01, -1.3587e-01, -3.1215e-01,\n",
              "          -1.6371e-01, -3.7734e-01, -2.1069e-01, -5.2719e-01, -7.2965e-01],\n",
              "         [ 3.0658e-02, -1.2151e-01, -1.1033e+00,  5.0581e-01,  5.7052e-01,\n",
              "          -5.8876e-01,  2.3847e-01,  6.6004e-01, -1.6753e-01,  1.6001e-01,\n",
              "           2.7275e-01, -7.5287e-01, -9.0958e-02, -8.6454e-01, -8.2452e-01],\n",
              "         [ 5.9473e-02, -1.4458e-01, -9.1056e-01,  2.8458e-01,  4.2134e-01,\n",
              "           8.6888e-02, -1.1115e-01, -2.5835e-01,  1.2345e-01, -1.3788e-01,\n",
              "           2.2245e-01, -3.7419e-01,  1.3231e-01, -3.5033e-01, -5.5887e-01],\n",
              "         [-1.6278e-01, -4.5313e-01, -1.6978e+00,  3.7104e-01,  3.2242e-02,\n",
              "          -1.7484e-01, -1.5598e-01, -3.8548e-01, -3.5344e-01, -5.2863e-01,\n",
              "           1.6512e-01, -6.0411e-01, -2.0002e-01, -8.1555e-01, -1.1269e+00],\n",
              "         [ 3.8604e-02, -3.6301e-01, -1.1427e+00,  5.5161e-01,  4.5333e-01,\n",
              "          -9.2791e-02, -1.0348e-01,  9.2674e-02, -1.7118e-01, -2.3996e-01,\n",
              "           1.0672e-01, -2.7215e-01,  3.2046e-02, -6.9695e-01, -3.4823e-01],\n",
              "         [-6.1204e-02, -3.6476e-01, -1.6603e+00,  4.1287e-01,  2.9819e-01,\n",
              "          -7.4908e-01, -4.7463e-01, -1.3428e-01, -5.9305e-01, -4.0641e-01,\n",
              "          -1.1933e-01, -8.2497e-01, -6.6090e-01, -6.1771e-01, -9.4389e-01],\n",
              "         [-1.9518e+00, -1.9881e+00, -3.0791e+00,  7.2595e-01, -1.3108e+00,\n",
              "          -2.3845e+00, -2.0787e+00, -1.3197e+00, -2.2131e+00, -1.8409e+00,\n",
              "          -2.0662e+00, -2.6564e+00, -2.1023e+00, -1.3154e+00, -1.8663e+00],\n",
              "         [-1.0724e+00, -8.1074e-01, -1.1205e+00, -4.2001e-01, -6.5877e-01,\n",
              "          -9.2244e-01, -7.3006e-01, -5.6062e-01, -8.7480e-01, -8.7711e-01,\n",
              "          -7.5156e-01, -8.8140e-01, -8.6288e-01, -1.1384e+00, -5.8159e-01],\n",
              "         [ 2.2188e-01, -4.4217e-01, -1.2220e+00,  3.7412e-01,  3.0537e-01,\n",
              "          -3.6621e-01, -2.6251e-01,  1.6799e-01, -9.1166e-01, -4.3798e-01,\n",
              "           2.0483e-01, -8.6023e-01, -1.6705e-01, -1.1990e+00, -1.1599e+00],\n",
              "         [ 3.0896e-01, -5.4126e-01, -1.0243e+00,  2.5229e-01, -4.9230e-01,\n",
              "          -9.2096e-01, -6.8456e-01,  3.4702e-02, -4.1789e-02,  9.7074e-02,\n",
              "          -4.2668e-01, -1.0829e+00, -8.4768e-01, -7.0463e-01, -9.4634e-01],\n",
              "         [-8.5963e-01, -7.0470e-01, -1.6123e+00,  5.2024e-01, -5.7922e-01,\n",
              "          -5.8529e-01, -5.3538e-01, -6.3676e-01, -2.7129e-01, -6.3444e-01,\n",
              "          -3.8853e-01, -5.5761e-01, -7.2781e-01, -3.5744e-01, -1.1323e+00],\n",
              "         [-2.6801e-01, -4.2337e-01, -4.9982e-01, -2.3262e-01,  1.5089e-01,\n",
              "          -1.0208e-02,  1.6834e-01, -3.2229e-01, -2.8398e-01, -4.4756e-01,\n",
              "           1.2333e-01, -8.7164e-03, -1.5119e-01, -7.1072e-01, -3.4289e-01],\n",
              "         [-1.6266e-01, -4.1182e-02, -5.5421e-01, -5.9772e-01, -7.9379e-02,\n",
              "          -3.5978e-01,  1.3145e-01, -6.7217e-02, -8.5916e-01, -3.3321e-01,\n",
              "           9.2692e-02, -1.5374e-01, -6.3436e-02, -5.2547e-01, -3.8091e-01],\n",
              "         [-2.0573e-01, -3.4153e-01, -4.3503e-01, -4.8565e-01, -2.5281e-01,\n",
              "          -2.8848e-01, -4.3013e-01,  1.1180e-01,  4.0062e-01,  3.8394e-01,\n",
              "          -1.0812e-01, -4.2032e-01, -1.8525e-01, -6.9523e-01, -1.6278e-01],\n",
              "         [-8.6574e-02, -1.9429e-01, -6.1516e-01, -1.3499e-01, -3.4473e-02,\n",
              "           1.0218e-01,  8.0897e-02, -9.0682e-02, -1.0480e-01, -1.5981e-01,\n",
              "          -3.9566e-04,  4.2075e-02,  2.3556e-01,  7.5976e-02, -3.1690e-01],\n",
              "         [-1.7485e-01, -4.4872e-01, -1.0761e+00,  3.8512e-01, -2.1172e-02,\n",
              "          -6.4442e-01, -1.9505e-01, -2.8225e-01, -1.8474e-01, -3.3263e-01,\n",
              "          -2.1671e-01, -6.9235e-01, -1.4961e-01, -4.6041e-01, -4.6623e-01],\n",
              "         [-7.1660e-01, -2.1028e-01, -1.0298e+00, -1.7088e-01, -4.6947e-02,\n",
              "          -1.4215e-01,  1.0702e-01,  4.5484e-02,  5.9748e-01,  4.3127e-01,\n",
              "          -1.4202e-01, -1.0358e-01, -3.8360e-01, -7.1925e-01, -8.7823e-01],\n",
              "         [ 5.4680e-02, -5.8337e-01, -1.3625e+00, -3.2458e-02, -1.7012e-01,\n",
              "          -2.0032e-01, -1.6696e-02, -2.0865e-02, -1.5302e-01,  4.0468e-02,\n",
              "          -4.3382e-01, -4.2921e-01, -4.6913e-01, -1.1536e+00, -1.3226e+00],\n",
              "         [ 2.8217e-02, -9.5582e-02, -1.3841e+00,  3.9087e-01,  4.5903e-01,\n",
              "          -3.6730e-01,  2.6417e-01,  3.5053e-01,  1.3479e-02,  2.3496e-01,\n",
              "           1.6965e-01, -5.1320e-01, -1.8965e-01, -7.7399e-01, -8.5834e-01],\n",
              "         [ 5.0295e-01, -6.2811e-02, -1.0456e+00,  6.6456e-01,  2.6503e-01,\n",
              "          -4.2443e-01, -7.4241e-02,  9.6454e-01, -1.1079e-03,  1.3687e-01,\n",
              "           2.1241e-01, -6.8468e-01, -1.8274e-01, -7.2373e-01, -1.0115e+00],\n",
              "         [-7.0774e-01, -1.4901e-01, -6.0831e-01, -5.1382e-01, -2.9590e-01,\n",
              "          -2.0587e-02, -2.4453e-01, -2.4190e-01,  2.7198e-01,  1.3735e-02,\n",
              "          -1.4270e-01,  8.3362e-02, -3.1807e-01, -6.4421e-01, -2.6217e-01],\n",
              "         [ 3.2448e-01, -1.0994e-01, -8.1553e-01, -4.1172e-01, -1.4537e-01,\n",
              "          -4.6630e-01, -3.4642e-01,  6.3396e-01,  5.2675e-01,  2.7899e-01,\n",
              "           5.2484e-02, -9.9052e-02, -2.9791e-02, -3.5848e-01, -4.9418e-01]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.4847, -0.3856, -0.3222, -1.2102, -0.7437, -0.3490, -0.3527, -0.0473,\n",
              "         -0.4554, -0.6909, -1.0504, -0.8170, -0.3106, -0.8938, -0.8891, -0.4496,\n",
              "         -0.8186, -0.6349, -0.6149, -0.3514, -1.2460, -0.6115, -0.6847, -0.3085,\n",
              "         -0.4351, -0.6492, -0.7101, -1.0237, -0.6803, -0.7102, -0.4928, -0.6980,\n",
              "         -0.3966, -0.6633, -1.1711, -1.1699, -1.0641, -0.8444, -0.2468, -0.5614,\n",
              "         -0.8095, -0.6788, -0.0199, -0.2181, -0.4412, -0.8439, -0.5854, -0.6572,\n",
              "         -0.4412, -0.6607], requires_grad=True), Parameter containing:\n",
              " tensor([[-9.3585e-01, -3.6031e-01, -8.4659e-01,  ..., -4.7884e-02,\n",
              "           7.4786e-02, -1.2171e-02],\n",
              "         [-4.4165e-01, -2.4383e-01, -3.7792e-01,  ..., -2.9511e-01,\n",
              "           1.2134e-01, -3.0762e-01],\n",
              "         [ 7.0609e-01,  2.0451e-01, -2.5559e-01,  ..., -3.1536e-01,\n",
              "           4.8989e-01,  5.4558e-03],\n",
              "         ...,\n",
              "         [ 3.0496e-01,  1.9424e+00,  3.5455e-01,  ..., -8.3223e-02,\n",
              "           8.3467e-02,  7.1800e-01],\n",
              "         [ 3.5565e-01, -5.6186e-02, -4.7922e-01,  ..., -1.0007e-01,\n",
              "          -9.0835e-04, -1.1079e-01],\n",
              "         [ 5.2440e-01, -2.8115e-01, -2.0737e-01,  ...,  1.0925e-01,\n",
              "           4.6244e-01, -2.4527e-03]], requires_grad=True), Parameter containing:\n",
              " tensor([-1.2353, -1.2481, -0.3475, -0.8376, -2.1566, -0.9887,  0.5688, -1.3492,\n",
              "         -0.6456,  0.3147, -0.9474, -0.6882, -0.6442, -1.5321, -0.4480, -0.7271,\n",
              "         -0.3472, -0.1899, -0.7115, -0.5237, -1.5647, -0.7626, -1.4617, -0.8149,\n",
              "         -0.1360, -0.9075, -1.2923, -1.2158, -0.7946, -0.5298, -0.4135, -1.3020,\n",
              "         -1.0698, -1.2788, -1.2259, -1.1449, -0.4058, -0.0990, -0.4384, -0.5453,\n",
              "         -1.4745, -1.1711, -0.8557, -0.6250, -0.7955, -1.0552, -0.9288, -0.2933,\n",
              "         -1.0095, -0.4401, -1.4755, -0.5604, -0.5364, -2.7860,  0.1952, -1.1491,\n",
              "         -0.8541, -0.5581, -1.2943, -1.4388, -1.1187, -1.1401, -1.2369, -0.2358,\n",
              "         -0.1275, -1.0725, -3.0565, -0.5563, -1.1043, -0.5699, -1.2880, -0.0234,\n",
              "         -0.0650, -0.9436, -0.1591, -0.6570, -0.9684, -1.1458, -0.9839, -0.5841,\n",
              "         -0.0561, -1.0790, -0.1268, -0.6953, -0.3223, -1.4712, -2.3822, -0.8614,\n",
              "         -0.7502, -1.2092, -1.0993, -1.2242, -0.4038, -0.9983, -0.6396, -0.0901,\n",
              "         -0.4675, -0.1967, -1.0594, -1.3805], requires_grad=True), Parameter containing:\n",
              " tensor([[-0.1370, -0.1848,  0.2483, -0.1064, -0.3374,  0.0664,  0.2283,  0.0357,\n",
              "          -0.0440,  0.2604,  0.1722,  0.2194, -0.0496,  0.1945, -0.1369,  0.1478,\n",
              "           0.0378, -0.2492, -0.1264,  0.0165,  0.2294,  0.4183, -0.2281,  0.0371,\n",
              "           0.0718,  0.1380,  0.1399,  0.2857,  0.2493, -0.0269,  0.1238,  0.3407,\n",
              "           0.2057, -0.0203,  0.1981,  0.1593, -0.1533,  0.3712, -0.0758, -0.1395,\n",
              "           0.0709,  0.0916,  0.2179,  0.1746, -0.0466, -0.0629,  0.2721, -0.0201,\n",
              "           0.2047, -0.0771,  0.0353,  0.1430,  0.0183, -0.3528,  0.3918, -0.0370,\n",
              "           0.0584, -0.0757, -0.2499,  0.0013,  0.2138, -0.0179, -0.1708, -0.0093,\n",
              "           0.1007,  0.0624, -0.4774, -0.0317,  0.0714,  0.3414,  0.1619, -0.2287,\n",
              "          -0.1314,  0.4019,  0.3547,  0.2299, -0.1205,  0.2263,  0.0135, -0.0026,\n",
              "          -0.0415,  0.2790,  0.2158, -0.0249,  0.5510,  0.1221, -0.3761,  0.2159,\n",
              "           0.1708,  0.1013,  0.0640,  0.1188,  0.2320, -0.2874, -0.1432,  0.0858,\n",
              "          -0.2948,  0.0162,  0.1280,  0.2234]], requires_grad=True), Parameter containing:\n",
              " tensor([3.5455], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22BL0f178lYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "89d42f2a-0bc6-4764-a3c9-732fa0678e49"
      },
      "source": [
        "net.modules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of Sequential(\n",
              "  (0): Linear(in_features=15, out_features=50, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.01)\n",
              "  (2): Linear(in_features=50, out_features=100, bias=True)\n",
              "  (3): LeakyReLU(negative_slope=0.01)\n",
              "  (4): Linear(in_features=100, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syU7GJ9L9SMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03032ed5-6ab5-432e-c12e-43dd47085c4e"
      },
      "source": [
        "net[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=15, out_features=50, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO7sQo8R9Yd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "366cb3a7-59e6-4248-9dba-4e75e4901211"
      },
      "source": [
        "list(net[0].parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 1.6080e-01, -4.7701e-01, -1.1910e+00,  3.7882e-01, -2.6354e-03,\n",
              "          -7.6638e-02, -1.4267e-01,  6.8461e-01,  3.2823e-01,  2.2673e-01,\n",
              "           1.0925e-02, -5.2119e-01, -2.9785e-01, -8.9852e-01, -1.1012e+00],\n",
              "         [-1.1339e+00, -1.2170e-01, -7.5101e-01, -1.2713e-01,  2.4377e-01,\n",
              "          -2.3888e-01, -1.7110e-01, -3.8567e-01, -7.3630e-01, -6.1386e-01,\n",
              "           8.9916e-02, -6.5075e-02,  4.1878e-02, -8.5701e-01, -5.6861e-01],\n",
              "         [-2.3527e-01,  2.9357e-02, -6.2764e-01,  3.6230e-01, -9.5669e-02,\n",
              "          -3.1904e-01,  2.1574e-01, -4.8795e-02, -2.2181e-01, -3.7442e-01,\n",
              "           2.6746e-01, -4.4817e-01,  3.9956e-02, -5.0411e-01, -5.1923e-01],\n",
              "         [-3.7311e-01, -2.3439e-01, -1.6399e+00,  2.5981e-01,  7.4721e-02,\n",
              "          -9.5946e-02, -2.9540e-02, -3.3854e-02,  2.6345e-01,  3.6847e-01,\n",
              "           2.0863e-01,  4.8608e-02, -1.7520e-01, -9.7328e-01, -1.2328e+00],\n",
              "         [-3.5793e-01, -1.7657e-01, -7.6195e-01, -3.6418e-01, -2.7561e-01,\n",
              "          -8.1255e-02, -1.1457e-01,  3.0267e-01,  1.7241e-01,  1.3706e-01,\n",
              "          -3.5185e-02, -1.4105e-01, -2.6749e-02, -6.9919e-01, -6.7252e-01],\n",
              "         [ 5.5714e-02, -1.7694e-01, -8.8868e-01, -9.7198e-02,  3.7384e-01,\n",
              "          -2.8970e-01, -1.5895e-01,  6.0742e-02, -3.3018e-01, -3.8827e-02,\n",
              "          -3.5266e-02, -4.7269e-01,  9.4689e-02, -7.0852e-01, -2.4678e-01],\n",
              "         [ 3.5770e-01, -4.7538e-01, -9.2766e-01, -2.3847e-01, -4.5348e-03,\n",
              "          -3.6074e-01, -2.9110e-02,  8.5592e-02, -5.7698e-01, -4.4207e-01,\n",
              "           2.6061e-01, -4.2877e-01, -1.1382e-01, -5.2981e-01, -5.4970e-01],\n",
              "         [ 8.9982e-02, -4.6809e-02, -4.4413e-01,  2.5067e-03,  9.0156e-02,\n",
              "          -5.7931e-02,  1.2840e-01,  2.8126e-01, -4.3314e-01, -1.2809e-01,\n",
              "           1.7230e-01, -1.2418e-02,  3.6374e-02, -4.2816e-01, -2.9245e-01],\n",
              "         [-3.1076e-01, -4.0213e-02, -3.0625e-01, -6.1701e-01,  2.1998e-01,\n",
              "          -4.5340e-02, -1.4302e-01, -3.9169e-02,  1.4611e-01,  3.5053e-01,\n",
              "          -9.3340e-02,  1.5172e-01,  3.1500e-01, -2.5684e-01, -2.8175e-01],\n",
              "         [-3.4603e-02,  1.2864e-01, -6.7875e-01, -4.6404e-01,  3.1646e-02,\n",
              "          -2.4888e-01,  2.8960e-02, -3.4269e-01, -2.4157e-01, -1.8609e-01,\n",
              "           1.6599e-01, -6.1054e-02,  1.1617e-01, -5.5788e-01, -5.4315e-01],\n",
              "         [-2.6240e-01, -1.1143e+00, -1.4008e+00, -4.2325e-02, -1.3136e+00,\n",
              "          -1.4152e+00, -1.3435e+00, -1.1282e+00, -9.6803e-01, -1.2438e+00,\n",
              "          -1.3517e+00, -1.2409e+00, -1.2016e+00, -8.0532e-01, -1.0973e+00],\n",
              "         [-6.3814e-01, -2.6736e-01, -1.1142e+00,  5.1602e-01,  2.4041e-01,\n",
              "          -5.3874e-01,  1.6936e-03,  3.6997e-02, -4.4345e-01, -8.9422e-02,\n",
              "           4.6983e-01, -4.1549e-01, -2.3554e-01, -1.0128e+00, -9.3580e-01],\n",
              "         [-8.8380e-01, -2.5893e-01, -1.0581e+00, -4.3448e-01, -1.3547e-01,\n",
              "           4.6382e-02, -3.1816e-01, -3.9476e-01, -3.5800e-01, -4.2356e-01,\n",
              "           5.2512e-02, -3.3528e-01, -2.7354e-01, -5.8010e-01, -3.4630e-01],\n",
              "         [ 7.3027e-01,  5.1470e-02, -1.3919e+00,  1.3748e-01,  3.7134e-01,\n",
              "          -6.7534e-02, -3.8328e-02,  4.6040e-01, -5.6559e-01, -7.5356e-02,\n",
              "           3.5826e-01, -3.5308e-01, -5.7734e-02, -7.6835e-01, -1.0805e+00],\n",
              "         [ 2.6757e-01, -2.2133e-01, -1.2312e+00,  4.6822e-01,  4.4557e-01,\n",
              "           3.7183e-02,  1.3306e-01, -2.1567e-02, -2.2422e-01,  1.0296e-01,\n",
              "          -7.5173e-02, -3.2706e-02, -3.5564e-01, -1.0494e+00, -8.6540e-01],\n",
              "         [-1.5654e-01, -1.3958e-01,  3.2398e-02, -1.0006e-01,  7.0712e-01,\n",
              "          -1.5769e-01, -1.3728e-01, -1.2561e-01, -1.2353e-01, -2.6710e-01,\n",
              "           5.1200e-01, -3.0642e-01,  2.1573e-01, -3.8753e-01, -6.4515e-01],\n",
              "         [ 4.4117e-01, -1.2353e-01, -1.5547e+00,  3.4476e-01,  2.3559e-01,\n",
              "          -2.8119e-01, -2.7663e-01,  7.3757e-01,  1.9485e-01,  6.9289e-01,\n",
              "           5.0324e-01, -5.1126e-01, -1.4423e-01, -6.9955e-01, -1.0542e+00],\n",
              "         [-3.4582e-01, -4.2744e-01, -8.0941e-01, -1.8099e-01, -3.1173e-01,\n",
              "          -2.4841e-01, -1.7844e-01, -6.2275e-01, -1.7863e-01, -4.4001e-01,\n",
              "          -5.1787e-01, -4.0971e-01, -6.6540e-01, -4.8946e-01, -8.3480e-01],\n",
              "         [-3.8379e-01, -3.0552e-01, -8.2250e-01, -1.9945e-01, -4.9965e-02,\n",
              "          -3.3494e-01, -3.1441e-01,  4.6990e-01, -7.3607e-05,  3.1155e-01,\n",
              "          -1.5737e-01, -2.9089e-01, -2.5357e-02, -5.8483e-01, -5.9078e-01],\n",
              "         [-6.2537e-01, -5.5105e-01, -8.5169e-01,  6.5175e-01,  1.6137e-02,\n",
              "          -3.5708e-01, -3.5139e-01, -5.6346e-01, -3.9152e-01, -6.0132e-01,\n",
              "          -2.5008e-01, -5.3926e-01, -2.4847e-01, -6.4181e-01, -7.1242e-01],\n",
              "         [-5.5261e-02, -9.3474e-01, -1.9334e+00,  1.0714e+00,  5.6620e-02,\n",
              "          -1.1418e+00, -8.5039e-01,  4.7520e-01, -1.1029e+00, -6.4421e-01,\n",
              "          -3.0979e-01, -1.4643e+00, -1.2180e+00, -1.3544e+00, -1.3515e+00],\n",
              "         [-6.0273e-01, -2.8107e-01, -1.0448e+00,  3.6862e-01,  2.3691e-01,\n",
              "          -3.8470e-01, -1.5118e-01, -2.2912e-01, -6.7316e-01, -3.6750e-01,\n",
              "          -2.9078e-02, -5.8989e-01, -1.7257e-01, -8.3200e-01, -5.3899e-01],\n",
              "         [-1.9427e-02, -1.1213e-01, -4.7661e-01, -8.4578e-01, -4.3447e-02,\n",
              "          -8.4524e-02, -1.6866e-01, -8.1856e-01, -6.8240e-01, -7.3316e-01,\n",
              "          -1.8231e-01, -2.0392e-01, -2.4811e-01, -1.6717e-01, -5.2461e-01],\n",
              "         [ 2.8450e-02, -3.2553e-01, -5.5800e-01, -1.7448e-01, -2.2453e-01,\n",
              "          -1.1772e-01, -1.3985e-01, -3.3425e-01, -4.7548e-01, -1.7629e-01,\n",
              "           6.7961e-02, -7.4095e-02, -1.6728e-01, -3.6018e-01, -4.0068e-01],\n",
              "         [ 2.1520e-01, -8.2232e-01, -9.2217e-01,  4.1003e-01, -5.0823e-02,\n",
              "          -6.1987e-01, -6.8105e-01, -1.0741e-01, -3.5566e-01, -3.6998e-01,\n",
              "          -5.2137e-01, -6.5521e-01, -5.4413e-01, -9.0036e-01, -9.5640e-01],\n",
              "         [-8.9493e-01, -1.9913e-01, -6.5909e-01, -2.2397e-01,  3.1595e-01,\n",
              "          -2.8345e-02, -6.4935e-02, -2.8850e-01, -8.1919e-01, -8.6805e-01,\n",
              "           3.3988e-01,  6.9819e-03,  1.1041e-01, -6.6253e-01, -5.5817e-01],\n",
              "         [-2.5169e-01, -2.9493e-01, -1.0375e+00, -2.4701e-01,  3.1599e-01,\n",
              "          -4.8711e-01, -8.7802e-02,  9.7531e-01,  8.5701e-02,  3.6209e-01,\n",
              "          -2.2561e-01, -4.5047e-01, -4.3691e-01, -1.0462e+00, -5.8032e-01],\n",
              "         [ 2.1586e-02, -5.8688e-01, -7.8982e-01, -7.9690e-01, -4.0544e-01,\n",
              "          -2.5066e-01, -1.7932e-01,  2.9271e-02,  5.3088e-01,  5.0646e-01,\n",
              "          -2.9115e-01, -4.6738e-01, -1.3056e-01, -1.0251e+00, -5.6702e-01],\n",
              "         [-5.6184e-01,  2.6206e-03, -1.0452e+00,  3.7767e-01,  3.4512e-02,\n",
              "           5.6537e-02, -1.3525e-01, -3.6690e-01, -1.3587e-01, -3.1215e-01,\n",
              "          -1.6371e-01, -3.7734e-01, -2.1069e-01, -5.2719e-01, -7.2965e-01],\n",
              "         [ 3.0658e-02, -1.2151e-01, -1.1033e+00,  5.0581e-01,  5.7052e-01,\n",
              "          -5.8876e-01,  2.3847e-01,  6.6004e-01, -1.6753e-01,  1.6001e-01,\n",
              "           2.7275e-01, -7.5287e-01, -9.0958e-02, -8.6454e-01, -8.2452e-01],\n",
              "         [ 5.9473e-02, -1.4458e-01, -9.1056e-01,  2.8458e-01,  4.2134e-01,\n",
              "           8.6888e-02, -1.1115e-01, -2.5835e-01,  1.2345e-01, -1.3788e-01,\n",
              "           2.2245e-01, -3.7419e-01,  1.3231e-01, -3.5033e-01, -5.5887e-01],\n",
              "         [-1.6278e-01, -4.5313e-01, -1.6978e+00,  3.7104e-01,  3.2242e-02,\n",
              "          -1.7484e-01, -1.5598e-01, -3.8548e-01, -3.5344e-01, -5.2863e-01,\n",
              "           1.6512e-01, -6.0411e-01, -2.0002e-01, -8.1555e-01, -1.1269e+00],\n",
              "         [ 3.8604e-02, -3.6301e-01, -1.1427e+00,  5.5161e-01,  4.5333e-01,\n",
              "          -9.2791e-02, -1.0348e-01,  9.2674e-02, -1.7118e-01, -2.3996e-01,\n",
              "           1.0672e-01, -2.7215e-01,  3.2046e-02, -6.9695e-01, -3.4823e-01],\n",
              "         [-6.1204e-02, -3.6476e-01, -1.6603e+00,  4.1287e-01,  2.9819e-01,\n",
              "          -7.4908e-01, -4.7463e-01, -1.3428e-01, -5.9305e-01, -4.0641e-01,\n",
              "          -1.1933e-01, -8.2497e-01, -6.6090e-01, -6.1771e-01, -9.4389e-01],\n",
              "         [-1.9518e+00, -1.9881e+00, -3.0791e+00,  7.2595e-01, -1.3108e+00,\n",
              "          -2.3845e+00, -2.0787e+00, -1.3197e+00, -2.2131e+00, -1.8409e+00,\n",
              "          -2.0662e+00, -2.6564e+00, -2.1023e+00, -1.3154e+00, -1.8663e+00],\n",
              "         [-1.0724e+00, -8.1074e-01, -1.1205e+00, -4.2001e-01, -6.5877e-01,\n",
              "          -9.2244e-01, -7.3006e-01, -5.6062e-01, -8.7480e-01, -8.7711e-01,\n",
              "          -7.5156e-01, -8.8140e-01, -8.6288e-01, -1.1384e+00, -5.8159e-01],\n",
              "         [ 2.2188e-01, -4.4217e-01, -1.2220e+00,  3.7412e-01,  3.0537e-01,\n",
              "          -3.6621e-01, -2.6251e-01,  1.6799e-01, -9.1166e-01, -4.3798e-01,\n",
              "           2.0483e-01, -8.6023e-01, -1.6705e-01, -1.1990e+00, -1.1599e+00],\n",
              "         [ 3.0896e-01, -5.4126e-01, -1.0243e+00,  2.5229e-01, -4.9230e-01,\n",
              "          -9.2096e-01, -6.8456e-01,  3.4702e-02, -4.1789e-02,  9.7074e-02,\n",
              "          -4.2668e-01, -1.0829e+00, -8.4768e-01, -7.0463e-01, -9.4634e-01],\n",
              "         [-8.5963e-01, -7.0470e-01, -1.6123e+00,  5.2024e-01, -5.7922e-01,\n",
              "          -5.8529e-01, -5.3538e-01, -6.3676e-01, -2.7129e-01, -6.3444e-01,\n",
              "          -3.8853e-01, -5.5761e-01, -7.2781e-01, -3.5744e-01, -1.1323e+00],\n",
              "         [-2.6801e-01, -4.2337e-01, -4.9982e-01, -2.3262e-01,  1.5089e-01,\n",
              "          -1.0208e-02,  1.6834e-01, -3.2229e-01, -2.8398e-01, -4.4756e-01,\n",
              "           1.2333e-01, -8.7164e-03, -1.5119e-01, -7.1072e-01, -3.4289e-01],\n",
              "         [-1.6266e-01, -4.1182e-02, -5.5421e-01, -5.9772e-01, -7.9379e-02,\n",
              "          -3.5978e-01,  1.3145e-01, -6.7217e-02, -8.5916e-01, -3.3321e-01,\n",
              "           9.2692e-02, -1.5374e-01, -6.3436e-02, -5.2547e-01, -3.8091e-01],\n",
              "         [-2.0573e-01, -3.4153e-01, -4.3503e-01, -4.8565e-01, -2.5281e-01,\n",
              "          -2.8848e-01, -4.3013e-01,  1.1180e-01,  4.0062e-01,  3.8394e-01,\n",
              "          -1.0812e-01, -4.2032e-01, -1.8525e-01, -6.9523e-01, -1.6278e-01],\n",
              "         [-8.6574e-02, -1.9429e-01, -6.1516e-01, -1.3499e-01, -3.4473e-02,\n",
              "           1.0218e-01,  8.0897e-02, -9.0682e-02, -1.0480e-01, -1.5981e-01,\n",
              "          -3.9566e-04,  4.2075e-02,  2.3556e-01,  7.5976e-02, -3.1690e-01],\n",
              "         [-1.7485e-01, -4.4872e-01, -1.0761e+00,  3.8512e-01, -2.1172e-02,\n",
              "          -6.4442e-01, -1.9505e-01, -2.8225e-01, -1.8474e-01, -3.3263e-01,\n",
              "          -2.1671e-01, -6.9235e-01, -1.4961e-01, -4.6041e-01, -4.6623e-01],\n",
              "         [-7.1660e-01, -2.1028e-01, -1.0298e+00, -1.7088e-01, -4.6947e-02,\n",
              "          -1.4215e-01,  1.0702e-01,  4.5484e-02,  5.9748e-01,  4.3127e-01,\n",
              "          -1.4202e-01, -1.0358e-01, -3.8360e-01, -7.1925e-01, -8.7823e-01],\n",
              "         [ 5.4680e-02, -5.8337e-01, -1.3625e+00, -3.2458e-02, -1.7012e-01,\n",
              "          -2.0032e-01, -1.6696e-02, -2.0865e-02, -1.5302e-01,  4.0468e-02,\n",
              "          -4.3382e-01, -4.2921e-01, -4.6913e-01, -1.1536e+00, -1.3226e+00],\n",
              "         [ 2.8217e-02, -9.5582e-02, -1.3841e+00,  3.9087e-01,  4.5903e-01,\n",
              "          -3.6730e-01,  2.6417e-01,  3.5053e-01,  1.3479e-02,  2.3496e-01,\n",
              "           1.6965e-01, -5.1320e-01, -1.8965e-01, -7.7399e-01, -8.5834e-01],\n",
              "         [ 5.0295e-01, -6.2811e-02, -1.0456e+00,  6.6456e-01,  2.6503e-01,\n",
              "          -4.2443e-01, -7.4241e-02,  9.6454e-01, -1.1079e-03,  1.3687e-01,\n",
              "           2.1241e-01, -6.8468e-01, -1.8274e-01, -7.2373e-01, -1.0115e+00],\n",
              "         [-7.0774e-01, -1.4901e-01, -6.0831e-01, -5.1382e-01, -2.9590e-01,\n",
              "          -2.0587e-02, -2.4453e-01, -2.4190e-01,  2.7198e-01,  1.3735e-02,\n",
              "          -1.4270e-01,  8.3362e-02, -3.1807e-01, -6.4421e-01, -2.6217e-01],\n",
              "         [ 3.2448e-01, -1.0994e-01, -8.1553e-01, -4.1172e-01, -1.4537e-01,\n",
              "          -4.6630e-01, -3.4642e-01,  6.3396e-01,  5.2675e-01,  2.7899e-01,\n",
              "           5.2484e-02, -9.9052e-02, -2.9791e-02, -3.5848e-01, -4.9418e-01]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.4847, -0.3856, -0.3222, -1.2102, -0.7437, -0.3490, -0.3527, -0.0473,\n",
              "         -0.4554, -0.6909, -1.0504, -0.8170, -0.3106, -0.8938, -0.8891, -0.4496,\n",
              "         -0.8186, -0.6349, -0.6149, -0.3514, -1.2460, -0.6115, -0.6847, -0.3085,\n",
              "         -0.4351, -0.6492, -0.7101, -1.0237, -0.6803, -0.7102, -0.4928, -0.6980,\n",
              "         -0.3966, -0.6633, -1.1711, -1.1699, -1.0641, -0.8444, -0.2468, -0.5614,\n",
              "         -0.8095, -0.6788, -0.0199, -0.2181, -0.4412, -0.8439, -0.5854, -0.6572,\n",
              "         -0.4412, -0.6607], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhKWfewZ9fPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2ef14c2-f5cb-4788-ae52-0bc58e4d4a82"
      },
      "source": [
        "net[0].weight.data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-QAkdw2-Wyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "463fb589-677d-436e-e03f-f908247cd859"
      },
      "source": [
        "net[0].weight.data[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1608, -0.4770, -1.1910,  0.3788, -0.0026, -0.0766, -0.1427,  0.6846,\n",
              "         0.3282,  0.2267,  0.0109, -0.5212, -0.2979, -0.8985, -1.1012])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nJPnjEw-gbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "12542081-ece6-4e56-f77c-d531ba34724b"
      },
      "source": [
        "data.columns[1:].tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['POS dist score',\n",
              " '1-gram_overlap',\n",
              " 'chrf_score_norm',\n",
              " 'WMD',\n",
              " 'ROUGE-1 recall',\n",
              " 'ROUGE-1 precision',\n",
              " 'ROUGE-1 F',\n",
              " 'ROUGE-2 recall',\n",
              " 'ROUGE-2 precision',\n",
              " 'ROUGE-2 F',\n",
              " 'ROUGE-L recall',\n",
              " 'ROUGE-L precision',\n",
              " 'ROUGE-L F',\n",
              " 'BertScore',\n",
              " 'L2_score']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8u9an8K-uD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_weights = pd.DataFrame(data.columns[1:].tolist(), columns = ['features']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPMia6xCc4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_weights[\"weights\"] = net[0].weight.data[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8l7PeyNCdq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "79a33c8b-8e1e-43be-8a7d-e65a3dd4399f"
      },
      "source": [
        "features_weights.sort_values(by=['weights'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chrf_score_norm</td>\n",
              "      <td>-1.190981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>L2_score</td>\n",
              "      <td>-1.101248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BertScore</td>\n",
              "      <td>-0.898518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ROUGE-L precision</td>\n",
              "      <td>-0.521188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-gram_overlap</td>\n",
              "      <td>-0.477014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ROUGE-L F</td>\n",
              "      <td>-0.297854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ROUGE-1 F</td>\n",
              "      <td>-0.142669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ROUGE-1 precision</td>\n",
              "      <td>-0.076638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ROUGE-1 recall</td>\n",
              "      <td>-0.002635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ROUGE-L recall</td>\n",
              "      <td>0.010925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POS dist score</td>\n",
              "      <td>0.160797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ROUGE-2 F</td>\n",
              "      <td>0.226734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ROUGE-2 precision</td>\n",
              "      <td>0.328227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WMD</td>\n",
              "      <td>0.378821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ROUGE-2 recall</td>\n",
              "      <td>0.684612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             features   weights\n",
              "2     chrf_score_norm -1.190981\n",
              "14           L2_score -1.101248\n",
              "13          BertScore -0.898518\n",
              "11  ROUGE-L precision -0.521188\n",
              "1      1-gram_overlap -0.477014\n",
              "12          ROUGE-L F -0.297854\n",
              "6           ROUGE-1 F -0.142669\n",
              "5   ROUGE-1 precision -0.076638\n",
              "4      ROUGE-1 recall -0.002635\n",
              "10     ROUGE-L recall  0.010925\n",
              "0      POS dist score  0.160797\n",
              "9           ROUGE-2 F  0.226734\n",
              "8   ROUGE-2 precision  0.328227\n",
              "3                 WMD  0.378821\n",
              "7      ROUGE-2 recall  0.684612"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs6PFIJHCjsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}