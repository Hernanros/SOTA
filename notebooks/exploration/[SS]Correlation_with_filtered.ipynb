{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitea4ac4decffd43fbb15ec47e3f649e63",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "from scipy.stats import pearsonr as pcorr\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# from src.features import metric_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = \"/home/shaul/workspace/GitHub/SOTA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/shaul/workspace/GitHub/SOTA\n"
    }
   ],
   "source": [
    "cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/shaul/workspace/GitHub/SOTA/data/combined/with_annotators/combined_dataset.csv', index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"annotator\" not in df.columns:\n",
    "    # Older version of the combined dataset with the annotator\n",
    "    df2 = pd.read_csv('/home/shaul/workspace/GitHub/SOTA/data/combined_dataset.csv.1', index_col = 0)\n",
    "    df['annotator'] = df2.annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_columns = ['text1','text2','label','dataset','random','duration','total_seconds','pair_id','reduced_label','annotator']\n",
    "\n",
    "def get_corr(df: pd.DataFrame, bad_annotator: list) -> dict:\n",
    "    '''\n",
    "    Get the correlation between the various metrics and the human labeling filtering out particular \"bad annotators\"\n",
    "\n",
    "    parameters:\n",
    "        df -- {pd.DataFrame} -- combined dataset\n",
    "        bad_annotator -- {list} -- list of all the annotator ID's we want to filter out\n",
    "\n",
    "    return:\n",
    "        {pd.DataFrame} - correlations by each dataset of metric and human label\n",
    "        {pd.DataFrame} - correlations by each dataset of metric and reduced human label (-1,0,1)\n",
    "        {pd.Series} - correlations of all datasets of metric and human label\n",
    "        {pd.Series} - correlations of all datasets of metris and reduced human label\n",
    "    '''\n",
    "\n",
    "    if bad_annotator:\n",
    "        df = df[~df.annotator.isin(bad_annotator)]\n",
    "        #Remove all pairs if there is only one annotator\n",
    "        df = df.groupby('pair_id').filter(lambda x: x.annotator.count() >= 2)\n",
    "\n",
    "    metrics = [x for x in df.columns if x not in non_metric_columns]\n",
    "    all_labels = metrics + ['label'] + ['reduced_label']\n",
    "    df = df.groupby(['pair_id','dataset','random'])[all_labels].mean().reset_index()\n",
    "\n",
    "    label_corr = dict()\n",
    "    reduced_label_corr = dict()\n",
    "\n",
    "    #Iterate through the datasets and get the correlation of each metric with label & reduced label (separately)\n",
    "    for name,group in df.groupby('dataset'):\n",
    "        label_corr[name] = group[metrics].corrwith(group['label'])\n",
    "        reduced_label_corr[name] = group[metrics].corrwith(group['reduced_label'])\n",
    "\n",
    "    combined_datasets_label_corr = df[metrics].corrwith(df['label'])\n",
    "    combined_datasets_reduced_label_corr = df[metrics].corrwith(df['reduced_label'])\n",
    "\n",
    "    random_label_corr = dict()\n",
    "    random_reduced_label_corr = dict()\n",
    "\n",
    "    for name,group in df.groupby('random'):\n",
    "        random_label_corr[name] = group[metrics].corrwith(group['label'])\n",
    "        random_reduced_label_corr[name] = group[metrics].corrwith(group['reduced_label'])\n",
    "\n",
    "    correlations_dict = dict()\n",
    "    correlations_dict['label_by_dataset'] = pd.DataFrame.from_dict(label_corr).T\n",
    "    correlations_dict['reduced_label_by_dataset'] = pd.DataFrame.from_dict(reduced_label_corr).T\n",
    "    correlations_dict['label_by_random'] = pd.DataFrame.from_dict(random_label_corr).T \n",
    "    correlations_dict['reduced_label_by_random'] = pd.DataFrame.from_dict(random_reduced_label_corr).T\n",
    "    correlations_dict['label_by_combined'] = pd.Series(combined_datasets_label_corr)\n",
    "    correlations_dict['reduced_label_by_combined'] = pd.Series(combined_datasets_reduced_label_corr)\n",
    "    return correlations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = get_corr(df,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                 bleu     bleu1  glove_cosine  \\\nbible_human                  0.301781  0.361418     -0.258113   \nbible_random_human           0.018227  0.081158     -0.058559   \ngyafc_formal_human           0.269344  0.345601     -0.263208   \ngyafc_formal_random_human    0.087416  0.064611     -0.064726   \ngyafc_informal_human         0.105411  0.252342     -0.137173   \ngyafc_informal_random_human  0.119732  0.052296      0.012553   \ngyafc_rewrites_human         0.257822  0.352715     -0.256443   \ngyafc_rewrites_random_human  0.006209  0.000246     -0.015786   \nparalex_human                0.146934  0.230694     -0.206681   \nparalex_random_human         0.036077  0.047361      0.000411   \nparaphrase_human             0.049653  0.236508     -0.164638   \nparaphrase_random_human      1.000000  0.032092     -0.052237   \nyelp_human                   0.147884  0.148415     -0.147229   \nyelp_random_human           -0.034052  0.056031     -0.089022   \n\n                             fasttext_cosine  BertScore  chrfScore  \\\nbible_human                        -0.303520   0.343436   0.349910   \nbible_random_human                 -0.074300   0.068136   0.045148   \ngyafc_formal_human                 -0.261788   0.372827   0.315667   \ngyafc_formal_random_human          -0.059007  -0.019708   0.116585   \ngyafc_informal_human               -0.145949   0.272675   0.237491   \ngyafc_informal_random_human        -0.010480   0.015643   0.067276   \ngyafc_rewrites_human               -0.248679   0.397966   0.361496   \ngyafc_rewrites_random_human        -0.039174   0.053218   0.012654   \nparalex_human                      -0.211279   0.352156   0.251824   \nparalex_random_human               -0.022915   0.014930   0.048666   \nparaphrase_human                   -0.252367   0.236215   0.239067   \nparaphrase_random_human            -0.006866   0.022278   0.104993   \nyelp_human                         -0.167460   0.142772   0.170866   \nyelp_random_human                  -0.076525   0.109966   0.071747   \n\n                             POS Dist score  1-gram_overlap   ROUGE-1  \\\nbible_human                       -0.249164        0.343448  0.359654   \nbible_random_human                -0.036966        0.049009  0.048639   \ngyafc_formal_human                -0.161177        0.335230  0.334441   \ngyafc_formal_random_human          0.007046        0.040632  0.054774   \ngyafc_informal_human              -0.138669        0.258827  0.242551   \ngyafc_informal_random_human       -0.035363        0.049867  0.044880   \ngyafc_rewrites_human              -0.211251        0.335940  0.350455   \ngyafc_rewrites_random_human        0.032680       -0.007742 -0.008345   \nparalex_human                     -0.045839        0.222752  0.236598   \nparalex_random_human              -0.023046        0.059672  0.057712   \nparaphrase_human                  -0.037471        0.233970  0.230518   \nparaphrase_random_human            0.050926        0.028566  0.033750   \nyelp_human                        -0.030932        0.154362  0.148052   \nyelp_random_human                 -0.031627        0.056564  0.043775   \n\n                              ROUGE-2   ROUGE-l  \nbible_human                  0.341609  0.362266  \nbible_random_human           0.028168  0.059865  \ngyafc_formal_human           0.290246  0.336569  \ngyafc_formal_random_human    0.036777  0.054641  \ngyafc_informal_human         0.194514  0.259640  \ngyafc_informal_random_human  0.070819  0.041311  \ngyafc_rewrites_human         0.305240  0.372787  \ngyafc_rewrites_random_human  0.003940 -0.016364  \nparalex_human                0.169609  0.241920  \nparalex_random_human         0.070155  0.060780  \nparaphrase_human             0.156814  0.233223  \nparaphrase_random_human           NaN  0.037671  \nyelp_human                   0.151306  0.146386  \nyelp_random_human            0.046280  0.046484  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bleu</th>\n      <th>bleu1</th>\n      <th>glove_cosine</th>\n      <th>fasttext_cosine</th>\n      <th>BertScore</th>\n      <th>chrfScore</th>\n      <th>POS Dist score</th>\n      <th>1-gram_overlap</th>\n      <th>ROUGE-1</th>\n      <th>ROUGE-2</th>\n      <th>ROUGE-l</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bible_human</th>\n      <td>0.301781</td>\n      <td>0.361418</td>\n      <td>-0.258113</td>\n      <td>-0.303520</td>\n      <td>0.343436</td>\n      <td>0.349910</td>\n      <td>-0.249164</td>\n      <td>0.343448</td>\n      <td>0.359654</td>\n      <td>0.341609</td>\n      <td>0.362266</td>\n    </tr>\n    <tr>\n      <th>bible_random_human</th>\n      <td>0.018227</td>\n      <td>0.081158</td>\n      <td>-0.058559</td>\n      <td>-0.074300</td>\n      <td>0.068136</td>\n      <td>0.045148</td>\n      <td>-0.036966</td>\n      <td>0.049009</td>\n      <td>0.048639</td>\n      <td>0.028168</td>\n      <td>0.059865</td>\n    </tr>\n    <tr>\n      <th>gyafc_formal_human</th>\n      <td>0.269344</td>\n      <td>0.345601</td>\n      <td>-0.263208</td>\n      <td>-0.261788</td>\n      <td>0.372827</td>\n      <td>0.315667</td>\n      <td>-0.161177</td>\n      <td>0.335230</td>\n      <td>0.334441</td>\n      <td>0.290246</td>\n      <td>0.336569</td>\n    </tr>\n    <tr>\n      <th>gyafc_formal_random_human</th>\n      <td>0.087416</td>\n      <td>0.064611</td>\n      <td>-0.064726</td>\n      <td>-0.059007</td>\n      <td>-0.019708</td>\n      <td>0.116585</td>\n      <td>0.007046</td>\n      <td>0.040632</td>\n      <td>0.054774</td>\n      <td>0.036777</td>\n      <td>0.054641</td>\n    </tr>\n    <tr>\n      <th>gyafc_informal_human</th>\n      <td>0.105411</td>\n      <td>0.252342</td>\n      <td>-0.137173</td>\n      <td>-0.145949</td>\n      <td>0.272675</td>\n      <td>0.237491</td>\n      <td>-0.138669</td>\n      <td>0.258827</td>\n      <td>0.242551</td>\n      <td>0.194514</td>\n      <td>0.259640</td>\n    </tr>\n    <tr>\n      <th>gyafc_informal_random_human</th>\n      <td>0.119732</td>\n      <td>0.052296</td>\n      <td>0.012553</td>\n      <td>-0.010480</td>\n      <td>0.015643</td>\n      <td>0.067276</td>\n      <td>-0.035363</td>\n      <td>0.049867</td>\n      <td>0.044880</td>\n      <td>0.070819</td>\n      <td>0.041311</td>\n    </tr>\n    <tr>\n      <th>gyafc_rewrites_human</th>\n      <td>0.257822</td>\n      <td>0.352715</td>\n      <td>-0.256443</td>\n      <td>-0.248679</td>\n      <td>0.397966</td>\n      <td>0.361496</td>\n      <td>-0.211251</td>\n      <td>0.335940</td>\n      <td>0.350455</td>\n      <td>0.305240</td>\n      <td>0.372787</td>\n    </tr>\n    <tr>\n      <th>gyafc_rewrites_random_human</th>\n      <td>0.006209</td>\n      <td>0.000246</td>\n      <td>-0.015786</td>\n      <td>-0.039174</td>\n      <td>0.053218</td>\n      <td>0.012654</td>\n      <td>0.032680</td>\n      <td>-0.007742</td>\n      <td>-0.008345</td>\n      <td>0.003940</td>\n      <td>-0.016364</td>\n    </tr>\n    <tr>\n      <th>paralex_human</th>\n      <td>0.146934</td>\n      <td>0.230694</td>\n      <td>-0.206681</td>\n      <td>-0.211279</td>\n      <td>0.352156</td>\n      <td>0.251824</td>\n      <td>-0.045839</td>\n      <td>0.222752</td>\n      <td>0.236598</td>\n      <td>0.169609</td>\n      <td>0.241920</td>\n    </tr>\n    <tr>\n      <th>paralex_random_human</th>\n      <td>0.036077</td>\n      <td>0.047361</td>\n      <td>0.000411</td>\n      <td>-0.022915</td>\n      <td>0.014930</td>\n      <td>0.048666</td>\n      <td>-0.023046</td>\n      <td>0.059672</td>\n      <td>0.057712</td>\n      <td>0.070155</td>\n      <td>0.060780</td>\n    </tr>\n    <tr>\n      <th>paraphrase_human</th>\n      <td>0.049653</td>\n      <td>0.236508</td>\n      <td>-0.164638</td>\n      <td>-0.252367</td>\n      <td>0.236215</td>\n      <td>0.239067</td>\n      <td>-0.037471</td>\n      <td>0.233970</td>\n      <td>0.230518</td>\n      <td>0.156814</td>\n      <td>0.233223</td>\n    </tr>\n    <tr>\n      <th>paraphrase_random_human</th>\n      <td>1.000000</td>\n      <td>0.032092</td>\n      <td>-0.052237</td>\n      <td>-0.006866</td>\n      <td>0.022278</td>\n      <td>0.104993</td>\n      <td>0.050926</td>\n      <td>0.028566</td>\n      <td>0.033750</td>\n      <td>NaN</td>\n      <td>0.037671</td>\n    </tr>\n    <tr>\n      <th>yelp_human</th>\n      <td>0.147884</td>\n      <td>0.148415</td>\n      <td>-0.147229</td>\n      <td>-0.167460</td>\n      <td>0.142772</td>\n      <td>0.170866</td>\n      <td>-0.030932</td>\n      <td>0.154362</td>\n      <td>0.148052</td>\n      <td>0.151306</td>\n      <td>0.146386</td>\n    </tr>\n    <tr>\n      <th>yelp_random_human</th>\n      <td>-0.034052</td>\n      <td>0.056031</td>\n      <td>-0.089022</td>\n      <td>-0.076525</td>\n      <td>0.109966</td>\n      <td>0.071747</td>\n      <td>-0.031627</td>\n      <td>0.056564</td>\n      <td>0.043775</td>\n      <td>0.046280</td>\n      <td>0.046484</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "test['label_by_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/shaul/workspace/GitHub/SOTA/data/other/ba_all.txt','r+') as f:\n",
    "    list_ba = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_baseline = get_corr(df,None)\n",
    "dict_filtered = get_corr(df,list_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_correlations(dict_baseline, dict_filtered):\n",
    "\n",
    "    ab_dict = dict()\n",
    "\n",
    "    for key in dict_baseline.keys():\n",
    "        ab_dict[key] = dict_filtered[key] - dict_baseline[key]\n",
    "\n",
    "    return ab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_dict = compare_correlations(dict_baseline,dict_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       bleu     bleu1  glove_cosine  fasttext_cosine  BertScore  chrfScore  \\\n0 -0.008881  0.007736     -0.018534        -0.037464  -0.003839   0.000465   \n1 -0.051604 -0.043984      0.034127         0.045167   0.006353  -0.050304   \n\n   POS Dist score  1-gram_overlap   ROUGE-1   ROUGE-2   ROUGE-l  \n0        0.030275       -0.004488  0.003180  0.007254  0.007926  \n1        0.005076       -0.048799 -0.038395 -0.049176 -0.030126  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bleu</th>\n      <th>bleu1</th>\n      <th>glove_cosine</th>\n      <th>fasttext_cosine</th>\n      <th>BertScore</th>\n      <th>chrfScore</th>\n      <th>POS Dist score</th>\n      <th>1-gram_overlap</th>\n      <th>ROUGE-1</th>\n      <th>ROUGE-2</th>\n      <th>ROUGE-l</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.008881</td>\n      <td>0.007736</td>\n      <td>-0.018534</td>\n      <td>-0.037464</td>\n      <td>-0.003839</td>\n      <td>0.000465</td>\n      <td>0.030275</td>\n      <td>-0.004488</td>\n      <td>0.003180</td>\n      <td>0.007254</td>\n      <td>0.007926</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.051604</td>\n      <td>-0.043984</td>\n      <td>0.034127</td>\n      <td>0.045167</td>\n      <td>0.006353</td>\n      <td>-0.050304</td>\n      <td>0.005076</td>\n      <td>-0.048799</td>\n      <td>-0.038395</td>\n      <td>-0.049176</td>\n      <td>-0.030126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "ab_dict['reduced_label_by_random']"
   ]
  },
  {
   "source": [
    "### Look at the Non-Linear and Linear Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [x for x in df.columns if x not in non_metric_columns]\n",
    "all_labels = metrics + ['label'] + ['reduced_label']\n",
    "\n",
    "df = df.groupby(['pair_id','dataset','random'])[all_labels].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop([\"label\",\"pair_id\",\"dataset\",\"random\",\"reduced_label\"], axis=1).copy()\n",
    "\n",
    "labels = df[\"label\"]\n",
    "labels_reduced = df['reduced_label']\n",
    "\n",
    "column_names = list(data.columns) \n",
    "x = data.values #returns a numpy array\n",
    "\n",
    "#scale the data values\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "data = pd.DataFrame(x_scaled, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF(df: pd.DataFrame, metrics: list)\n",
    "\n",
    "    all_labels = metrics + ['label'] + ['reduced_label']\n",
    "\n",
    "    df = df.groupby(['pair_id','dataset','random'])[all_labels].mean().reset_index()\n",
    "    data = df.drop(['label','pair_id\",\"dataset','random','reduced_label'], axis=1).copy()\n",
    "\n",
    "    #TODO: This hasn't been written to explore the scores grouped by dataset and random\n",
    "\n",
    "    labels = df[\"label\"]\n",
    "    labels_reduced = df['reduced_label']\n",
    "\n",
    "    column_names = list(data.columns) \n",
    "    x = data.values #returns a numpy array\n",
    "\n",
    "    #scale the data values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    data = pd.DataFrame(x_scaled, columns=column_names)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(data, labels_reduced, test_size=0.2)\n",
    "\n",
    "    scores = dict()\n",
    "\n",
    "    #Get the score from the models\n",
    "\n",
    "    model1 = RandomForestRegressor(max_depth=3)\n",
    "    model1.fit(X_train, y_train)\n",
    "    y_pred = model1.predict(X_test)\n",
    "    scores['combined_RF_label'] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    model2 = RandomForestRegressor(max_depth=3)\n",
    "    model2.fit(X_train_reduced, y_train_reduced)\n",
    "    y_pred_reduced = model2.predict(X_test_reduced)\n",
    "    scores['combined_RF_label_reduced'] = mean_squared_error(y_test_reduced, y_pred_reduced)\n",
    "\n",
    "    feature_importance_dict = {'feature': data.columns.values, 'importance':model1.feature_importances_} \n",
    "    feature_importance_df['combined_RF_features'] = pd.DataFrame(feature_importance_dict).sort_values('importance', ascending=False) \n",
    "\n",
    "    #Repeat for each of the datasets and by random"
   ]
  }
 ]
}